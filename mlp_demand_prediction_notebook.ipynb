{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLP Demand Prediction (Task 2 & Task 3)\n",
        "\n",
        "Este notebook carga el fichero `bike_data_clean.parquet` generado en `clustering_parquet_file.ipynb`\n",
        "y ejecuta el modelo **MLP (Multi-Layer Perceptron)** para predecir la demanda horaria\n",
        "(pickups y dropoffs) para los próximos 24h en distintos clusters, además de calcular\n",
        "el número de bicicletas requeridas al inicio del día (Task 3).\n",
        "\n",
        "Requisitos previos:\n",
        "- Ejecutar antes el notebook `clustering_parquet_file.ipynb` para que exista `bike_data_clean.parquet`.\n",
        "- Tener instaladas las librerías: `pandas`, `numpy`, `matplotlib`, `scikit-learn`, `pyarrow`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Ruta del fichero Parquet generado en el notebook de clustering\n",
        "DATA_PATH = Path(\"bike_data_clean.parquet\")\n",
        "\n",
        "# Clusters a modelar (puedes cambiar esta lista)\n",
        "CLUSTERS_TO_MODEL = [0, 1]\n",
        "\n",
        "# Mostrar todas las figuras inline\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# ------------------------------------------------------------\n",
        "# Utilidades para construir la demanda horaria por cluster\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def build_hourly_demand_for_cluster(df: pd.DataFrame, cluster_id: int) -> pd.DataFrame:\n",
        "    \"\"\"Construye una tabla de demanda horaria para un cluster concreto.\n",
        "\n",
        "    Salida: DataFrame con columnas:\n",
        "        - date (datetime)\n",
        "        - hour (0..23)\n",
        "        - pickups  (viajes que empiezan en este cluster)\n",
        "        - dropoffs (viajes que terminan en este cluster)\n",
        "        - cluster_id\n",
        "    \"\"\"\n",
        "    df_pick = df[df[\"gmm20_cluster\"] == cluster_id]\n",
        "    df_drop = df[df[\"end_gmm20_cluster\"] == cluster_id]\n",
        "\n",
        "    if df_pick.empty and df_drop.empty:\n",
        "        raise ValueError(f\"No hay datos de viajes para el cluster {cluster_id}\")\n",
        "\n",
        "    # --- Agregamos pickups: origen en este cluster ---\n",
        "    pick_group = (\n",
        "        df_pick\n",
        "        .groupby([\"start_date\", \"start_hour\"])\n",
        "        .size()\n",
        "        .rename(\"pickups\")\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # --- Agregamos dropoffs: destino en este cluster ---\n",
        "    drop_group = (\n",
        "        df_drop\n",
        "        .groupby([\"stop_date\", \"stop_hour\"])\n",
        "        .size()\n",
        "        .rename(\"dropoffs\")\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # Renombramos columnas a un formato común (date, hour)\n",
        "    pick_group = pick_group.rename(\n",
        "        columns={\"start_date\": \"date\", \"start_hour\": \"hour\"}\n",
        "    )\n",
        "    drop_group = drop_group.rename(\n",
        "        columns={\"stop_date\": \"date\", \"stop_hour\": \"hour\"}\n",
        "    )\n",
        "\n",
        "    # Normalizamos las fechas a día\n",
        "    pick_group[\"date\"] = pd.to_datetime(pick_group[\"date\"]).dt.normalize()\n",
        "    drop_group[\"date\"] = pd.to_datetime(drop_group[\"date\"]).dt.normalize()\n",
        "\n",
        "    # Rango completo de fechas y horas a cubrir (rellenamos con 0 cuando no hay viajes)\n",
        "    min_date = min(pick_group[\"date\"].min(), drop_group[\"date\"].min())\n",
        "    max_date = max(pick_group[\"date\"].max(), drop_group[\"date\"].max())\n",
        "    all_dates = pd.date_range(min_date, max_date, freq=\"D\")\n",
        "    hours = np.arange(24)\n",
        "\n",
        "    # Creamos un índice de todas las combinaciones (fecha, hora)\n",
        "    idx = pd.MultiIndex.from_product([all_dates, hours], names=[\"date\", \"hour\"])\n",
        "    hourly = pd.DataFrame(index=idx).reset_index()\n",
        "\n",
        "    # Hacemos join con las tablas de pickups y dropoffs\n",
        "    hourly = hourly.merge(pick_group, on=[\"date\", \"hour\"], how=\"left\")\n",
        "    hourly = hourly.merge(drop_group, on=[\"date\", \"hour\"], how=\"left\")\n",
        "\n",
        "    # Rellenamos NaN con 0 (no hubo viajes esa hora)\n",
        "    hourly[\"pickups\"] = hourly[\"pickups\"].fillna(0).astype(\"float32\")\n",
        "    hourly[\"dropoffs\"] = hourly[\"dropoffs\"].fillna(0).astype(\"float32\")\n",
        "\n",
        "    hourly[\"cluster_id\"] = cluster_id\n",
        "\n",
        "    return hourly\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Dataset supervisado (día D -> día D+1)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def build_supervised_from_hourly(\n",
        "    hourly_df: pd.DataFrame,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, pd.DatetimeIndex]:\n",
        "    \"\"\"Construye el dataset supervisado a nivel de día.\n",
        "\n",
        "    Para cada día d:\n",
        "        X_d  = [24h pickups(d), 24h dropoffs(d), weekday(d+1), month(d+1)]\n",
        "        y_d  = 24h pickups(d+1)\n",
        "        y2_d = 24h dropoffs(d+1)\n",
        "    \"\"\"\n",
        "    # Pivot a matrices día x hora\n",
        "    daily_p = hourly_df.pivot(index=\"date\", columns=\"hour\", values=\"pickups\")\n",
        "    daily_d = hourly_df.pivot(index=\"date\", columns=\"hour\", values=\"dropoffs\")\n",
        "\n",
        "    # Nos aseguramos de que existen todas las columnas 0..23\n",
        "    for h in range(24):\n",
        "        if h not in daily_p.columns:\n",
        "            daily_p[h] = 0.0\n",
        "        if h not in daily_d.columns:\n",
        "            daily_d[h] = 0.0\n",
        "\n",
        "    daily_p = daily_p[sorted(daily_p.columns)]\n",
        "    daily_d = daily_d[sorted(daily_d.columns)]\n",
        "\n",
        "    # Ordenamos por fecha\n",
        "    dates = daily_p.index.sort_values()\n",
        "    daily_p = daily_p.loc[dates]\n",
        "    daily_d = daily_d.loc[dates]\n",
        "\n",
        "    X_list: List[np.ndarray] = []\n",
        "    Yp_list: List[np.ndarray] = []\n",
        "    Yd_list: List[np.ndarray] = []\n",
        "    pred_dates: List[pd.Timestamp] = []\n",
        "\n",
        "    for i in range(len(dates) - 1):\n",
        "        d = dates[i]\n",
        "        d_next = dates[i + 1]\n",
        "\n",
        "        # Features: 24h pickups(d) + 24h dropoffs(d) + calendario de d+1\n",
        "        X_vec = np.concatenate(\n",
        "            [\n",
        "                daily_p.loc[d].values.astype(\"float32\"),\n",
        "                daily_d.loc[d].values.astype(\"float32\"),\n",
        "                np.array([d_next.weekday(), d_next.month], dtype=\"float32\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Targets: 24h del día siguiente\n",
        "        y_pick = daily_p.loc[d_next].values.astype(\"float32\")\n",
        "        y_drop = daily_d.loc[d_next].values.astype(\"float32\")\n",
        "\n",
        "        X_list.append(X_vec)\n",
        "        Yp_list.append(y_pick)\n",
        "        Yd_list.append(y_drop)\n",
        "        pred_dates.append(d_next)\n",
        "\n",
        "    X = np.stack(X_list)\n",
        "    Yp = np.stack(Yp_list)\n",
        "    Yd = np.stack(Yd_list)\n",
        "    pred_dates = pd.to_datetime(pred_dates)\n",
        "\n",
        "    return X, Yp, Yd, pred_dates\n",
        "\n",
        "\n",
        "def split_train_test(\n",
        "    X: np.ndarray,\n",
        "    Yp: np.ndarray,\n",
        "    Yd: np.ndarray,\n",
        "    pred_dates: pd.DatetimeIndex,\n",
        "):\n",
        "    \"\"\"Split temporal train/test:\n",
        "\n",
        "    - Train: fechas predichas < 2018-11-01 (ene–oct)\n",
        "    - Test : fechas predichas >= 2018-11-01 (nov–dic)\n",
        "    \"\"\"\n",
        "    cutoff = pd.to_datetime(\"2018-11-01\")\n",
        "    train_mask = pred_dates < cutoff\n",
        "    test_mask = pred_dates >= cutoff\n",
        "\n",
        "    X_train = X[train_mask]\n",
        "    X_test = X[test_mask]\n",
        "    Yp_train = Yp[train_mask]\n",
        "    Yp_test = Yp[test_mask]\n",
        "    Yd_train = Yd[train_mask]\n",
        "    Yd_test = Yd[test_mask]\n",
        "    dates_train = pred_dates[train_mask]\n",
        "    dates_test = pred_dates[test_mask]\n",
        "\n",
        "    return X_train, X_test, Yp_train, Yp_test, Yd_train, Yd_test, dates_train, dates_test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# ------------------------------------------------------------\n",
        "# MLP + Task 3 (bicis requeridas)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def train_mlp(X_train: np.ndarray, Y_train: np.ndarray):\n",
        "    \"\"\"Entrena un MLPRegressor para regresión multi-salida.\n",
        "\n",
        "    Devuelve el modelo y el scaler usado en las features.\n",
        "    \"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "    mlp = MLPRegressor(\n",
        "        hidden_layer_sizes=(64, 32),\n",
        "        activation=\"relu\",\n",
        "        solver=\"adam\",\n",
        "        learning_rate_init=0.001,\n",
        "        max_iter=500,\n",
        "        random_state=42,\n",
        "        early_stopping=True,\n",
        "        n_iter_no_change=10,\n",
        "        validation_fraction=0.1,\n",
        "    )\n",
        "\n",
        "    mlp.fit(X_train_scaled, Y_train)\n",
        "    return mlp, scaler\n",
        "\n",
        "\n",
        "def compute_required_bikes_series(Yp: np.ndarray, Yd: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Task 3: calcula el nº de bicis requeridas al inicio del día.\n",
        "\n",
        "    net_t = dropoffs_t - pickups_t\n",
        "    cum_net_k = sum_{t=0..k} net_t\n",
        "    required_bikes = max(0, -min_k cum_net_k)\n",
        "    \"\"\"\n",
        "    required = []\n",
        "    for pickups, dropoffs in zip(Yp, Yd):\n",
        "        net = dropoffs - pickups\n",
        "        cum_net = np.cumsum(net)\n",
        "        min_cum = float(cum_net.min())\n",
        "        req = max(0.0, -min_cum)\n",
        "        required.append(req)\n",
        "    return np.array(required, dtype=\"float32\")\n",
        "\n",
        "\n",
        "def evaluate_cluster_with_mlp(\n",
        "    df: pd.DataFrame,\n",
        "    cluster_id: int,\n",
        "):\n",
        "    \"\"\"Pipeline completo para un cluster:\n",
        "      - Demanda horaria\n",
        "      - Dataset supervisado día -> día+1\n",
        "      - Split temporal\n",
        "      - Entrenamiento de dos MLP (pickups y dropoffs)\n",
        "      - Evaluación\n",
        "      - Cálculo de bicis requeridas (Task 3)\n",
        "      - Gráficos principales\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"CLUSTER {cluster_id} - Construyendo demanda horaria ...\")\n",
        "    hourly = build_hourly_demand_for_cluster(df, cluster_id)\n",
        "    print(f\"[INFO] hourly shape: {hourly.shape}\")\n",
        "\n",
        "    print(\"[INFO] Construyendo dataset supervisado día -> día+1 ...\")\n",
        "    X, Yp, Yd, pred_dates = build_supervised_from_hourly(hourly)\n",
        "    print(f\"[INFO] Número de muestras (días-1): {X.shape[0]}\")\n",
        "\n",
        "    print(\"[INFO] Split temporal (train = ene–oct, test = nov–dic) ...\")\n",
        "    (\n",
        "        X_train,\n",
        "        X_test,\n",
        "        Yp_train,\n",
        "        Yp_test,\n",
        "        Yd_train,\n",
        "        Yd_test,\n",
        "        dates_train,\n",
        "        dates_test,\n",
        "    ) = split_train_test(X, Yp, Yd, pred_dates)\n",
        "\n",
        "    print(f\"  Train samples: {X_train.shape[0]}\")\n",
        "    print(f\"  Test samples : {X_test.shape[0]}\")\n",
        "\n",
        "    if X_test.shape[0] == 0:\n",
        "        print(\"[WARN] No hay días en el test set para este cluster (nov–dic). Se omite.\")\n",
        "        return\n",
        "\n",
        "    # --------------------- Entrenamiento MLP ---------------------\n",
        "    print(\"[INFO] Entrenando MLP para pickups ...\")\n",
        "    mlp_pickups, scaler_p = train_mlp(X_train, Yp_train)\n",
        "    X_test_scaled_p = scaler_p.transform(X_test)\n",
        "    Yp_pred_test = mlp_pickups.predict(X_test_scaled_p)\n",
        "\n",
        "    print(\"[INFO] Entrenando MLP para dropoffs ...\")\n",
        "    mlp_dropoffs, scaler_d = train_mlp(X_train, Yd_train)\n",
        "    X_test_scaled_d = scaler_d.transform(X_test)\n",
        "    Yd_pred_test = mlp_dropoffs.predict(X_test_scaled_d)\n",
        "\n",
        "    # --------------------- Métricas ---------------------\n",
        "    rmse_p = np.sqrt(mean_squared_error(Yp_test.ravel(), Yp_pred_test.ravel()))\n",
        "    mae_p = mean_absolute_error(Yp_test.ravel(), Yp_pred_test.ravel())\n",
        "    rmse_d = np.sqrt(mean_squared_error(Yd_test.ravel(), Yd_pred_test.ravel()))\n",
        "    mae_d = mean_absolute_error(Yd_test.ravel(), Yd_pred_test.ravel())\n",
        "\n",
        "    print(f\"\\n[RESULTADOS] Cluster {cluster_id}\")\n",
        "    print(f\"  Pickups  - RMSE={rmse_p:.3f}, MAE={mae_p:.3f}\")\n",
        "    print(f\"  Dropoffs - RMSE={rmse_d:.3f}, MAE={mae_d:.3f}\")\n",
        "\n",
        "    # --------------------- Task 3: Bicis requeridas ---------------------\n",
        "    print(\"[INFO] Calculando bicis requeridas (Task 3) con predicciones y datos reales ...\")\n",
        "    required_pred = compute_required_bikes_series(Yp_pred_test, Yd_pred_test)\n",
        "    required_true = compute_required_bikes_series(Yp_test, Yd_test)\n",
        "\n",
        "    df_req = pd.DataFrame(\n",
        "        {\n",
        "            \"date\": dates_test,\n",
        "            \"required_bikes_true\": required_true,\n",
        "            \"required_bikes_pred\": required_pred,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(\"\\n[Task 3] Ejemplo de bicis requeridas (primeros 10 días de test):\")\n",
        "    print(df_req.head(10))\n",
        "\n",
        "    # --------------------- Gráficos ---------------------\n",
        "    hours = np.arange(24)\n",
        "\n",
        "    # 1) Ejemplo de día de test (primer día)\n",
        "    idx0 = 0\n",
        "    date0 = dates_test[idx0].date()\n",
        "\n",
        "    plt.figure(figsize=(9, 5))\n",
        "    plt.plot(hours, Yp_test[idx0], marker=\"o\", label=\"Actual pickups\")\n",
        "    plt.plot(hours, Yp_pred_test[idx0], marker=\"x\", label=\"Predicted pickups\")\n",
        "    plt.xlabel(\"Hour\")\n",
        "    plt.ylabel(\"Demand\")\n",
        "    plt.title(f\"Cluster {cluster_id} - Pickups - {date0}\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.figure(figsize=(9, 5))\n",
        "    plt.plot(hours, Yd_test[idx0], marker=\"o\", label=\"Actual dropoffs\")\n",
        "    plt.plot(hours, Yd_pred_test[idx0], marker=\"x\", label=\"Predicted dropoffs\")\n",
        "    plt.xlabel(\"Hour\")\n",
        "    plt.ylabel(\"Demand\")\n",
        "    plt.title(f\"Cluster {cluster_id} - Dropoffs - {date0}\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # 2) Bicis requeridas en varios días de test (barras)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    x = np.arange(min(10, len(df_req)))  # como mucho 10 días para visualizar\n",
        "\n",
        "    plt.bar(\n",
        "        x - 0.15,\n",
        "        df_req[\"required_bikes_true\"].values[: len(x)],\n",
        "        width=0.3,\n",
        "        label=\"True\",\n",
        "    )\n",
        "    plt.bar(\n",
        "        x + 0.15,\n",
        "        df_req[\"required_bikes_pred\"].values[: len(x)],\n",
        "        width=0.3,\n",
        "        label=\"Predicted\",\n",
        "    )\n",
        "\n",
        "    # Etiquetas de fecha\n",
        "    date_labels = df_req[\"date\"].dt.strftime(\"%Y-%m-%d\").values[: len(x)]\n",
        "    plt.xticks(x, date_labels, rotation=45)\n",
        "\n",
        "    plt.ylabel(\"Required bikes at start of day\")\n",
        "    plt.title(f\"Cluster {cluster_id} - Required bikes (Task 3) - First test days\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# ------------------------------------------------------------\n",
        "# Ejecución del pipeline MLP\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"MLP DEMAND PREDICTION (Task 2 & Task 3)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"[INFO] Leyendo datos desde {DATA_PATH.resolve()} ...\")\n",
        "\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"No se ha encontrado {DATA_PATH}. \"\n",
        "        \"Ejecuta antes el notebook de clustering para generar bike_data_clean.parquet.\"\n",
        "    )\n",
        "\n",
        "df = pd.read_parquet(DATA_PATH)\n",
        "\n",
        "print(f\"[INFO] DataFrame shape: {df.shape}\")\n",
        "print(\"[INFO] Columnas disponibles (primeras 20):\")\n",
        "print(df.columns[:20])\n",
        "\n",
        "for cluster_id in CLUSTERS_TO_MODEL:\n",
        "    try:\n",
        "        evaluate_cluster_with_mlp(df, cluster_id)\n",
        "    except ValueError as e:\n",
        "        print(f\"[WARN] Cluster {cluster_id}: {e}\")\n"
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLP Demand Prediction (Task 2 & Task 3)\n",
        "\n",
        "Este notebook carga el fichero `bike_data_clean.parquet` generado en `clustering_parquet_file.ipynb`\n",
        "y ejecuta el modelo **MLP (Multi-Layer Perceptron)** para predecir la demanda horaria\n",
        "(pickups y dropoffs) para los próximos 24h en distintos clusters, además de calcular\n",
        "el número de bicicletas requeridas al inicio del día (Task 3).\n",
        "\n",
        "Requisitos previos:\n",
        "- Ejecutar antes el notebook `clustering_parquet_file.ipynb` para que exista `bike_data_clean.parquet`.\n",
        "- Tener instaladas las librerías: `pandas`, `numpy`, `matplotlib`, `scikit-learn`, `pyarrow`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Data\n",
        "DATA_PATH = Path(\"bike_data_clean.parquet\")\n",
        "\n",
        "# Clusters\n",
        "CLUSTERS_TO_MODEL = [0, 1]\n",
        "\n",
        "# Splits (based on pred_dates = day being predicted)\n",
        "VAL_START = pd.to_datetime(\"2018-10-01\")\n",
        "VAL_END   = pd.to_datetime(\"2018-11-01\")  # exclusive\n",
        "TEST_START = pd.to_datetime(\"2018-11-01\")\n",
        "\n",
        "# Output folders\n",
        "ARTIFACTS_DIR = Path(\"artifacts_mlp\")\n",
        "PRED_DIR = Path(\"preds_mlp\")\n",
        "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ------------------------------------------------------------\n",
        "# Utilidades para construir la demanda horaria por cluster\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def build_hourly_demand_for_cluster(df: pd.DataFrame, cluster_id: int) -> pd.DataFrame:\n",
        "    \"\"\"Construye una tabla de demanda horaria para un cluster concreto.\n",
        "\n",
        "    Salida: DataFrame con columnas:\n",
        "        - date (datetime)\n",
        "        - hour (0..23)\n",
        "        - pickups  (viajes que empiezan en este cluster)\n",
        "        - dropoffs (viajes que terminan en este cluster)\n",
        "        - cluster_id\n",
        "    \"\"\"\n",
        "    df_pick = df[df[\"gmm20_cluster\"] == cluster_id]\n",
        "    df_drop = df[df[\"end_gmm20_cluster\"] == cluster_id]\n",
        "\n",
        "    if df_pick.empty and df_drop.empty:\n",
        "        raise ValueError(f\"No hay datos de viajes para el cluster {cluster_id}\")\n",
        "\n",
        "    # --- Agregamos pickups: origen en este cluster ---\n",
        "    pick_group = (\n",
        "        df_pick\n",
        "        .groupby([\"start_date\", \"start_hour\"])\n",
        "        .size()\n",
        "        .rename(\"pickups\")\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # --- Agregamos dropoffs: destino en este cluster ---\n",
        "    drop_group = (\n",
        "        df_drop\n",
        "        .groupby([\"stop_date\", \"stop_hour\"])\n",
        "        .size()\n",
        "        .rename(\"dropoffs\")\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # Renombramos columnas a un formato común (date, hour)\n",
        "    pick_group = pick_group.rename(\n",
        "        columns={\"start_date\": \"date\", \"start_hour\": \"hour\"}\n",
        "    )\n",
        "    drop_group = drop_group.rename(\n",
        "        columns={\"stop_date\": \"date\", \"stop_hour\": \"hour\"}\n",
        "    )\n",
        "\n",
        "    # Normalizamos las fechas a día\n",
        "    pick_group[\"date\"] = pd.to_datetime(pick_group[\"date\"]).dt.normalize()\n",
        "    drop_group[\"date\"] = pd.to_datetime(drop_group[\"date\"]).dt.normalize()\n",
        "\n",
        "    # Rango completo de fechas y horas a cubrir (rellenamos con 0 cuando no hay viajes)\n",
        "    min_date = min(pick_group[\"date\"].min(), drop_group[\"date\"].min())\n",
        "    max_date = max(pick_group[\"date\"].max(), drop_group[\"date\"].max())\n",
        "    all_dates = pd.date_range(min_date, max_date, freq=\"D\")\n",
        "    hours = np.arange(24)\n",
        "\n",
        "    # Creamos un índice de todas las combinaciones (fecha, hora)\n",
        "    idx = pd.MultiIndex.from_product([all_dates, hours], names=[\"date\", \"hour\"])\n",
        "    hourly = pd.DataFrame(index=idx).reset_index()\n",
        "\n",
        "    # Hacemos join con las tablas de pickups y dropoffs\n",
        "    hourly = hourly.merge(pick_group, on=[\"date\", \"hour\"], how=\"left\")\n",
        "    hourly = hourly.merge(drop_group, on=[\"date\", \"hour\"], how=\"left\")\n",
        "\n",
        "    # Rellenamos NaN con 0 (no hubo viajes esa hora)\n",
        "    hourly[\"pickups\"] = hourly[\"pickups\"].fillna(0).astype(\"float32\")\n",
        "    hourly[\"dropoffs\"] = hourly[\"dropoffs\"].fillna(0).astype(\"float32\")\n",
        "\n",
        "    hourly[\"cluster_id\"] = cluster_id\n",
        "\n",
        "    return hourly\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Dataset supervisado (día D -> día D+1)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def build_supervised_from_hourly(\n",
        "    hourly_df: pd.DataFrame,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, pd.DatetimeIndex]:\n",
        "    \"\"\"Construye el dataset supervisado a nivel de día.\n",
        "\n",
        "    Para cada día d:\n",
        "        X_d  = [24h pickups(d), 24h dropoffs(d), weekday(d+1), month(d+1)]\n",
        "        y_d  = 24h pickups(d+1)\n",
        "        y2_d = 24h dropoffs(d+1)\n",
        "    \"\"\"\n",
        "    # Pivot a matrices día x hora\n",
        "    daily_p = hourly_df.pivot(index=\"date\", columns=\"hour\", values=\"pickups\")\n",
        "    daily_d = hourly_df.pivot(index=\"date\", columns=\"hour\", values=\"dropoffs\")\n",
        "\n",
        "    # Nos aseguramos de que existen todas las columnas 0..23\n",
        "    for h in range(24):\n",
        "        if h not in daily_p.columns:\n",
        "            daily_p[h] = 0.0\n",
        "        if h not in daily_d.columns:\n",
        "            daily_d[h] = 0.0\n",
        "\n",
        "    daily_p = daily_p[sorted(daily_p.columns)]\n",
        "    daily_d = daily_d[sorted(daily_d.columns)]\n",
        "\n",
        "    # Ordenamos por fecha\n",
        "    dates = daily_p.index.sort_values()\n",
        "    daily_p = daily_p.loc[dates]\n",
        "    daily_d = daily_d.loc[dates]\n",
        "\n",
        "    X_list: List[np.ndarray] = []\n",
        "    Yp_list: List[np.ndarray] = []\n",
        "    Yd_list: List[np.ndarray] = []\n",
        "    pred_dates: List[pd.Timestamp] = []\n",
        "\n",
        "    for i in range(len(dates) - 1):\n",
        "        d = dates[i]\n",
        "        d_next = dates[i + 1]\n",
        "\n",
        "        # Features: 24h pickups(d) + 24h dropoffs(d) + calendario de d+1\n",
        "        X_vec = np.concatenate(\n",
        "            [\n",
        "                daily_p.loc[d].values.astype(\"float32\"),\n",
        "                daily_d.loc[d].values.astype(\"float32\"),\n",
        "                np.array([d_next.weekday(), d_next.month], dtype=\"float32\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Targets: 24h del día siguiente\n",
        "        y_pick = daily_p.loc[d_next].values.astype(\"float32\")\n",
        "        y_drop = daily_d.loc[d_next].values.astype(\"float32\")\n",
        "\n",
        "        X_list.append(X_vec)\n",
        "        Yp_list.append(y_pick)\n",
        "        Yd_list.append(y_drop)\n",
        "        pred_dates.append(d_next)\n",
        "\n",
        "    X = np.stack(X_list)\n",
        "    Yp = np.stack(Yp_list)\n",
        "    Yd = np.stack(Yd_list)\n",
        "    pred_dates = pd.to_datetime(pred_dates)\n",
        "\n",
        "    return X, Yp, Yd, pred_dates\n",
        "\n",
        "\n",
        "def split_train_val_test(\n",
        "    X: np.ndarray,\n",
        "    Yp: np.ndarray,\n",
        "    Yd: np.ndarray,\n",
        "    pred_dates: pd.DatetimeIndex,\n",
        "    val_start=VAL_START,\n",
        "    val_end=VAL_END,\n",
        "    test_start=TEST_START,\n",
        "):\n",
        "    \"\"\"\n",
        "    Temporal split based on the day being predicted (pred_dates):\n",
        "\n",
        "    - Train: pred_dates < val_start            (Jan–Sep)\n",
        "    - Val  : val_start <= pred_dates < val_end (Oct)\n",
        "    - Test : pred_dates >= test_start          (Nov–Dec)\n",
        "    \"\"\"\n",
        "    pred_dates = pd.to_datetime(pred_dates)\n",
        "\n",
        "    train_mask = pred_dates < val_start\n",
        "    val_mask   = (pred_dates >= val_start) & (pred_dates < val_end)\n",
        "    test_mask  = pred_dates >= test_start\n",
        "\n",
        "    return (\n",
        "        X[train_mask], X[val_mask], X[test_mask],\n",
        "        Yp[train_mask], Yp[val_mask], Yp[test_mask],\n",
        "        Yd[train_mask], Yd[val_mask], Yd[test_mask],\n",
        "        pred_dates[train_mask], pred_dates[val_mask], pred_dates[test_mask],\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_mlp(random_state=42) -> MLPRegressor:\n",
        "    return MLPRegressor(\n",
        "        hidden_layer_sizes=(64, 32),\n",
        "        activation=\"relu\",\n",
        "        solver=\"adam\",\n",
        "        learning_rate_init=0.001,\n",
        "        max_iter=500,\n",
        "        random_state=random_state,\n",
        "        early_stopping=True,\n",
        "        n_iter_no_change=10,\n",
        "        validation_fraction=0.1,\n",
        "    )\n",
        "\n",
        "def fit_model_with_scaler(X_train: np.ndarray, Y_train: np.ndarray, random_state=42):\n",
        "    scaler = StandardScaler()\n",
        "    Xs = scaler.fit_transform(X_train)\n",
        "    model = build_mlp(random_state=random_state)\n",
        "    model.fit(Xs, Y_train)\n",
        "    return model, scaler\n",
        "\n",
        "def predict_with(model, scaler, X):\n",
        "    return model.predict(scaler.transform(X))\n",
        "\n",
        "def to_long_df(dates, cluster_id, Yp_true, Yp_pred, Yd_true, Yd_pred, split_name: str):\n",
        "    n_days = len(dates)\n",
        "    hours = np.arange(24)\n",
        "\n",
        "    df_out = pd.DataFrame({\n",
        "        \"date\": np.repeat(dates, 24),\n",
        "        \"hour\": np.tile(hours, n_days),\n",
        "        \"cluster_id\": cluster_id,\n",
        "        \"split\": split_name,\n",
        "        \"y_true_pickups\":  Yp_true.reshape(-1),\n",
        "        \"y_pred_mlp_pickups\": Yp_pred.reshape(-1),\n",
        "        \"y_true_dropoffs\": Yd_true.reshape(-1),\n",
        "        \"y_pred_mlp_dropoffs\": Yd_pred.reshape(-1),\n",
        "    })\n",
        "    # Absolute errors per target\n",
        "    df_out[\"ae_pickups_mlp\"]  = (df_out[\"y_true_pickups\"]  - df_out[\"y_pred_mlp_pickups\"]).abs()\n",
        "    df_out[\"ae_dropoffs_mlp\"] = (df_out[\"y_true_dropoffs\"] - df_out[\"y_pred_mlp_dropoffs\"]).abs()\n",
        "    # Optional combined absolute error (useful for global weighting)\n",
        "    df_out[\"ae_mean_mlp\"] = 0.5 * (df_out[\"ae_pickups_mlp\"] + df_out[\"ae_dropoffs_mlp\"])\n",
        "    return df_out\n",
        "\n",
        "def evaluate_cluster_with_mlp_and_export(df: pd.DataFrame, cluster_id: int) -> Dict:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"CLUSTER {cluster_id} - building hourly demand ...\")\n",
        "    hourly = build_hourly_demand_for_cluster(df, cluster_id)\n",
        "\n",
        "    print(\"[INFO] building supervised dataset (day D -> day D+1) ...\")\n",
        "    X, Yp, Yd, pred_dates = build_supervised_from_hourly(hourly)\n",
        "\n",
        "    (\n",
        "        X_train, X_val, X_test,\n",
        "        Yp_train, Yp_val, Yp_test,\n",
        "        Yd_train, Yd_val, Yd_test,\n",
        "        dates_train, dates_val, dates_test,\n",
        "    ) = split_train_val_test(X, Yp, Yd, pred_dates)\n",
        "\n",
        "    print(f\"[INFO] Samples - train: {len(dates_train)}, val(Oct): {len(dates_val)}, test(Nov-Dec): {len(dates_test)}\")\n",
        "    if len(dates_val) == 0:\n",
        "        raise ValueError(\"Validation set is empty (October). Check date ranges in your dataset.\")\n",
        "    if len(dates_test) == 0:\n",
        "        print(\"[WARN] Test set is empty (Nov-Dec). Will still compute validation MAE and save artifacts.\")\n",
        "\n",
        "    # =========================\n",
        "    # 1) Train on TRAIN, evaluate on VAL (MAE for weighting)\n",
        "    # =========================\n",
        "    print(\"[INFO] Training (train-only) -> validating on October to compute MAE ...\")\n",
        "\n",
        "    mlp_p_tr, sc_p_tr = fit_model_with_scaler(X_train, Yp_train, random_state=42)\n",
        "    mlp_d_tr, sc_d_tr = fit_model_with_scaler(X_train, Yd_train, random_state=42)\n",
        "\n",
        "    Yp_val_pred = predict_with(mlp_p_tr, sc_p_tr, X_val)\n",
        "    Yd_val_pred = predict_with(mlp_d_tr, sc_d_tr, X_val)\n",
        "\n",
        "    mae_val_pickups  = mean_absolute_error(Yp_val.reshape(-1), Yp_val_pred.reshape(-1))\n",
        "    mae_val_dropoffs = mean_absolute_error(Yd_val.reshape(-1), Yd_val_pred.reshape(-1))\n",
        "    mae_val_mean = 0.5 * (mae_val_pickups + mae_val_dropoffs)\n",
        "\n",
        "    print(f\"[VAL MAE] Cluster {cluster_id}: pickups={mae_val_pickups:.4f} | dropoffs={mae_val_dropoffs:.4f} | mean={mae_val_mean:.4f}\")\n",
        "\n",
        "    # =========================\n",
        "    # 2) Retrain final model on TRAIN+VAL (Jan–Oct), then predict TEST\n",
        "    # =========================\n",
        "    print(\"[INFO] Training final models on train+val (Jan–Oct) ...\")\n",
        "    X_trval = np.vstack([X_train, X_val])\n",
        "    Yp_trval = np.vstack([Yp_train, Yp_val])\n",
        "    Yd_trval = np.vstack([Yd_train, Yd_val])\n",
        "\n",
        "    mlp_p_final, sc_p_final = fit_model_with_scaler(X_trval, Yp_trval, random_state=42)\n",
        "    mlp_d_final, sc_d_final = fit_model_with_scaler(X_trval, Yd_trval, random_state=42)\n",
        "\n",
        "    # Save artifacts\n",
        "    artifact = {\n",
        "        \"cluster_id\": cluster_id,\n",
        "        \"val_mae_pickups\": float(mae_val_pickups),\n",
        "        \"val_mae_dropoffs\": float(mae_val_dropoffs),\n",
        "        \"val_mae_mean\": float(mae_val_mean),\n",
        "        \"model_pickups\": mlp_p_final,\n",
        "        \"scaler_pickups\": sc_p_final,\n",
        "        \"model_dropoffs\": mlp_d_final,\n",
        "        \"scaler_dropoffs\": sc_d_final,\n",
        "        \"splits\": {\n",
        "            \"val_start\": str(VAL_START.date()),\n",
        "            \"val_end\": str(VAL_END.date()),\n",
        "            \"test_start\": str(TEST_START.date()),\n",
        "        }\n",
        "    }\n",
        "    joblib_path = ARTIFACTS_DIR / f\"mlp_cluster_{cluster_id}.joblib\"\n",
        "    joblib.dump(artifact, joblib_path)\n",
        "    print(f\"[INFO] Saved joblib: {joblib_path}\")\n",
        "\n",
        "    # =========================\n",
        "    # 3) Export per-hour predictions + ground truths for VAL and TEST\n",
        "    # =========================\n",
        "    dfs = []\n",
        "\n",
        "    # VAL preds (use train-only models, so the parquet reflects the same setup used for MAE)\n",
        "    df_val = to_long_df(\n",
        "        dates_val, cluster_id,\n",
        "        Yp_val, Yp_val_pred,\n",
        "        Yd_val, Yd_val_pred,\n",
        "        split_name=\"val\"\n",
        "    )\n",
        "    dfs.append(df_val)\n",
        "\n",
        "    # TEST preds (use final models trained on train+val)\n",
        "    if len(dates_test) > 0:\n",
        "        Yp_test_pred = predict_with(mlp_p_final, sc_p_final, X_test)\n",
        "        Yd_test_pred = predict_with(mlp_d_final, sc_d_final, X_test)\n",
        "\n",
        "        df_test = to_long_df(\n",
        "            dates_test, cluster_id,\n",
        "            Yp_test, Yp_test_pred,\n",
        "            Yd_test, Yd_test_pred,\n",
        "            split_name=\"test\"\n",
        "        )\n",
        "        dfs.append(df_test)\n",
        "\n",
        "    df_preds = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    parquet_path = PRED_DIR / f\"mlp_cluster_{cluster_id}_preds.parquet\"\n",
        "    df_preds.to_parquet(parquet_path, index=False)\n",
        "    print(f\"[INFO] Saved parquet: {parquet_path}  shape={df_preds.shape}\")\n",
        "\n",
        "    # Return summary (useful to aggregate later)\n",
        "    return {\n",
        "        \"cluster_id\": cluster_id,\n",
        "        \"val_mae_pickups\": float(mae_val_pickups),\n",
        "        \"val_mae_dropoffs\": float(mae_val_dropoffs),\n",
        "        \"val_mae_mean\": float(mae_val_mean),\n",
        "        \"joblib_path\": str(joblib_path),\n",
        "        \"parquet_path\": str(parquet_path),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "MLP DEMAND PREDICTION (Train/Val/Test + Export + Weight)\n",
            "================================================================================\n",
            "[INFO] DataFrame shape: (17531179, 22)\n",
            "\n",
            "================================================================================\n",
            "CLUSTER 0 - building hourly demand ...\n",
            "[INFO] building supervised dataset (day D -> day D+1) ...\n",
            "[INFO] Samples - train: 272, val(Oct): 31, test(Nov-Dec): 62\n",
            "[INFO] Training (train-only) -> validating on October to compute MAE ...\n",
            "[VAL MAE] Cluster 0: pickups=13.0729 | dropoffs=9.6840 | mean=11.3784\n",
            "[INFO] Training final models on train+val (Jan–Oct) ...\n",
            "[INFO] Saved joblib: artifacts_mlp/mlp_cluster_0.joblib\n",
            "[INFO] Saved parquet: preds_mlp/mlp_cluster_0_preds.parquet  shape=(2232, 11)\n",
            "\n",
            "================================================================================\n",
            "CLUSTER 1 - building hourly demand ...\n",
            "[INFO] building supervised dataset (day D -> day D+1) ...\n",
            "[INFO] Samples - train: 272, val(Oct): 31, test(Nov-Dec): 61\n",
            "[INFO] Training (train-only) -> validating on October to compute MAE ...\n",
            "[VAL MAE] Cluster 1: pickups=33.9725 | dropoffs=33.0744 | mean=33.5235\n",
            "[INFO] Training final models on train+val (Jan–Oct) ...\n",
            "[INFO] Saved joblib: artifacts_mlp/mlp_cluster_1.joblib\n",
            "[INFO] Saved parquet: preds_mlp/mlp_cluster_1_preds.parquet  shape=(2208, 11)\n",
            "\n",
            "Per-cluster October validation MAE summary:\n",
            "   cluster_id  val_mae_pickups  val_mae_dropoffs  val_mae_mean\n",
            "0           0        13.072922          9.683973     11.378448\n",
            "1           1        33.972481         33.074429     33.523455\n",
            "\n",
            "[INFO] MLP general October MAE: 22.450951\n",
            "[INFO] MLP raw weight (inverse MAE): 0.044542\n",
            "[INFO] Saved weight info: artifacts_mlp/mlp_general_weight.json\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"MLP DEMAND PREDICTION (Train/Val/Test + Export + Weight)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Missing {DATA_PATH}. Run clustering notebook first.\")\n",
        "\n",
        "df = pd.read_parquet(DATA_PATH)\n",
        "print(f\"[INFO] DataFrame shape: {df.shape}\")\n",
        "\n",
        "summaries = []\n",
        "for cluster_id in CLUSTERS_TO_MODEL:\n",
        "    try:\n",
        "        summary = evaluate_cluster_with_mlp_and_export(df, cluster_id)\n",
        "        summaries.append(summary)\n",
        "    except ValueError as e:\n",
        "        print(f\"[WARN] Cluster {cluster_id}: {e}\")\n",
        "\n",
        "df_summary = pd.DataFrame(summaries)\n",
        "print(\"\\nPer-cluster October validation MAE summary:\")\n",
        "print(df_summary[[\"cluster_id\", \"val_mae_pickups\", \"val_mae_dropoffs\", \"val_mae_mean\"]])\n",
        "\n",
        "# ---- A single \"general MAE\" for the MLP model across your selected clusters ----\n",
        "if len(df_summary) > 0:\n",
        "    general_mae_mlp = float(df_summary[\"val_mae_mean\"].mean())\n",
        "    eps = 1e-6\n",
        "    general_weight_mlp = 1.0 / (general_mae_mlp + eps)  # unnormalized weight; normalize later across models\n",
        "\n",
        "    weights_info = {\n",
        "        \"model\": \"MLP\",\n",
        "        \"clusters\": CLUSTERS_TO_MODEL,\n",
        "        \"general_mae_val_oct\": general_mae_mlp,\n",
        "        \"general_weight_raw\": general_weight_mlp,\n",
        "        \"val_start\": str(VAL_START.date()),\n",
        "        \"val_end\": str(VAL_END.date()),\n",
        "    }\n",
        "    weights_path = ARTIFACTS_DIR / \"mlp_general_weight.json\"\n",
        "    import json\n",
        "    weights_path.write_text(json.dumps(weights_info, indent=2))\n",
        "    print(f\"\\n[INFO] MLP general October MAE: {general_mae_mlp:.6f}\")\n",
        "    print(f\"[INFO] MLP raw weight (inverse MAE): {general_weight_mlp:.6f}\")\n",
        "    print(f\"[INFO] Saved weight info: {weights_path}\")\n",
        "else:\n",
        "    print(\"[WARN] No clusters produced results; cannot compute general MAE/weight.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82dda40c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deep_learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc3a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# ===== INPUTS =====\n",
    "DATA_PATH = Path(\"bike_data_clean.parquet\")\n",
    "\n",
    "# Your pretrained regressors (pickups + dropoffs)\n",
    "DEP_MODEL_PATH = Path(\"alex_ridge_model.joblib\")  # pickups model\n",
    "ARR_MODEL_PATH = Path(\"alex_ridge_model_dropoffs.joblib\")    # dropoffs model\n",
    "\n",
    "CLUSTERS_TO_MODEL = [0, 1]\n",
    "\n",
    "# Weights from October\n",
    "VAL_START = pd.to_datetime(\"2018-10-01\")\n",
    "VAL_END   = pd.to_datetime(\"2018-11-01\")  # exclusive\n",
    "TEST_START = pd.to_datetime(\"2018-11-01\")\n",
    "\n",
    "# Feature columns used in training (from your code)\n",
    "NUM_FEATS = ['lag_1','lag_24','lag_168','rmean_3h','rmean_24h']\n",
    "CAT_FEATS = ['hour','weekday','gmm20_cluster']\n",
    "FEATURES = NUM_FEATS + CAT_FEATS\n",
    "\n",
    "# Lag/rolling history needed before Oct 1\n",
    "# Need at least 168 hours + 24 hours + 1 hour buffer -> ~8 days is safe\n",
    "HISTORY_BUFFER_DAYS = 10\n",
    "\n",
    "# We'll build data only up to end of Dec\n",
    "END_TIME = pd.Timestamp(\"2018-12-31 23:00:00\")\n",
    "\n",
    "# ===== OUTPUTS =====\n",
    "ARTIFACTS_DIR = Path(\"artifacts_regressor\")\n",
    "PRED_DIR = Path(\"preds_regressor\")\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PRED_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83a70268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class DenseTransformer(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        try:\n",
    "            return X.toarray()\n",
    "        except Exception:\n",
    "            return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a28c80fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Departures model: <class 'sklearn.pipeline.Pipeline'>\n",
      "[INFO] Arrivals model  : <class 'sklearn.pipeline.Pipeline'>\n"
     ]
    }
   ],
   "source": [
    "assert DEP_MODEL_PATH.exists(), f\"Missing: {DEP_MODEL_PATH}\"\n",
    "assert ARR_MODEL_PATH.exists(), f\"Missing: {ARR_MODEL_PATH}\"\n",
    "\n",
    "dep_model = joblib.load(DEP_MODEL_PATH)\n",
    "arr_model = joblib.load(ARR_MODEL_PATH)\n",
    "\n",
    "print(\"[INFO] Departures model:\", type(dep_model))\n",
    "print(\"[INFO] Arrivals model  :\", type(arr_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e3141dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using time window: 2018-09-21 00:00:00 to 2018-12-31 23:00:00\n",
      "[INFO] Filtered df shape: (1540295, 6)\n",
      "[INFO] Columns: ['gmm20_cluster', 'end_gmm20_cluster', 'start_date', 'start_hour', 'stop_date', 'stop_hour']\n"
     ]
    }
   ],
   "source": [
    "# Time window to load: (Oct 1 - buffer) ... Dec 31\n",
    "START_TIME = (VAL_START - pd.Timedelta(days=HISTORY_BUFFER_DAYS)).floor(\"D\")\n",
    "print(\"[INFO] Using time window:\", START_TIME, \"to\", END_TIME)\n",
    "\n",
    "# Read only necessary columns\n",
    "needed_cols = [\"gmm20_cluster\",\"end_gmm20_cluster\",\"start_date\",\"start_hour\",\"stop_date\",\"stop_hour\"]\n",
    "df = pd.read_parquet(DATA_PATH, columns=needed_cols)\n",
    "\n",
    "clusters = set(CLUSTERS_TO_MODEL)\n",
    "mask = df[\"gmm20_cluster\"].isin(clusters) | df[\"end_gmm20_cluster\"].isin(clusters)\n",
    "df = df.loc[mask].copy()\n",
    "\n",
    "print(\"[INFO] Filtered df shape:\", df.shape)\n",
    "print(\"[INFO] Columns:\", list(df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3360bb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] agg_pick shape: (4729, 3)\n",
      "[INFO] agg_drop shape: (4784, 3)\n",
      "   gmm20_cluster           timestamp  pickups\n",
      "0              0 2018-09-21 00:00:00       18\n",
      "1              0 2018-09-21 01:00:00        8\n",
      "2              0 2018-09-21 02:00:00        9\n",
      "3              0 2018-09-21 03:00:00        3\n",
      "4              0 2018-09-21 05:00:00        2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_612636/373820492.py:11: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  pick_tbl[\"timestamp\"] = pd.to_datetime(pick_tbl[\"timestamp\"]).dt.floor(\"H\")\n",
      "/tmp/ipykernel_612636/373820492.py:26: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  drop_tbl[\"timestamp\"] = pd.to_datetime(drop_tbl[\"timestamp\"]).dt.floor(\"H\")\n"
     ]
    }
   ],
   "source": [
    "# Build hourly timestamps (vectorized)\n",
    "start_ts = pd.to_datetime(df[\"start_date\"]).dt.normalize() + pd.to_timedelta(df[\"start_hour\"], unit=\"h\")\n",
    "stop_ts  = pd.to_datetime(df[\"stop_date\"]).dt.normalize()  + pd.to_timedelta(df[\"stop_hour\"], unit=\"h\")\n",
    "\n",
    "# Pickups aggregation\n",
    "pick_mask = df[\"gmm20_cluster\"].isin(clusters)\n",
    "pick_tbl = pd.DataFrame({\n",
    "    \"gmm20_cluster\": df.loc[pick_mask, \"gmm20_cluster\"].astype(\"int16\").values,\n",
    "    \"timestamp\": start_ts.loc[pick_mask].values\n",
    "})\n",
    "pick_tbl[\"timestamp\"] = pd.to_datetime(pick_tbl[\"timestamp\"]).dt.floor(\"H\")\n",
    "pick_tbl = pick_tbl[(pick_tbl[\"timestamp\"] >= START_TIME) & (pick_tbl[\"timestamp\"] <= END_TIME)]\n",
    "agg_pick = (\n",
    "    pick_tbl.groupby([\"gmm20_cluster\",\"timestamp\"])\n",
    "    .size()\n",
    "    .rename(\"pickups\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Dropoffs aggregation\n",
    "drop_mask = df[\"end_gmm20_cluster\"].isin(clusters)\n",
    "drop_tbl = pd.DataFrame({\n",
    "    \"gmm20_cluster\": df.loc[drop_mask, \"end_gmm20_cluster\"].astype(\"int16\").values,\n",
    "    \"timestamp\": stop_ts.loc[drop_mask].values\n",
    "})\n",
    "drop_tbl[\"timestamp\"] = pd.to_datetime(drop_tbl[\"timestamp\"]).dt.floor(\"H\")\n",
    "drop_tbl = drop_tbl[(drop_tbl[\"timestamp\"] >= START_TIME) & (drop_tbl[\"timestamp\"] <= END_TIME)]\n",
    "agg_drop = (\n",
    "    drop_tbl.groupby([\"gmm20_cluster\",\"timestamp\"])\n",
    "    .size()\n",
    "    .rename(\"dropoffs\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"[INFO] agg_pick shape:\", agg_pick.shape)\n",
    "print(\"[INFO] agg_drop shape:\", agg_drop.shape)\n",
    "print(agg_pick.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6750dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ts_pick shape: (4896, 3) ts_drop shape: (4896, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_612636/2965832457.py:2: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  hours = pd.date_range(start, end, freq=\"H\")\n"
     ]
    }
   ],
   "source": [
    "def make_hour_grid(clusters_list, start, end):\n",
    "    hours = pd.date_range(start, end, freq=\"H\")\n",
    "    grid = pd.MultiIndex.from_product([clusters_list, hours], names=[\"gmm20_cluster\",\"timestamp\"]).to_frame(index=False)\n",
    "    return grid\n",
    "\n",
    "grid = make_hour_grid(CLUSTERS_TO_MODEL, START_TIME, END_TIME)\n",
    "\n",
    "ts_pick = grid.merge(agg_pick, on=[\"gmm20_cluster\",\"timestamp\"], how=\"left\")\n",
    "ts_pick[\"pickups\"] = ts_pick[\"pickups\"].fillna(0).astype(\"float32\")\n",
    "\n",
    "ts_drop = grid.merge(agg_drop, on=[\"gmm20_cluster\",\"timestamp\"], how=\"left\")\n",
    "ts_drop[\"dropoffs\"] = ts_drop[\"dropoffs\"].fillna(0).astype(\"float32\")\n",
    "\n",
    "print(\"[INFO] ts_pick shape:\", ts_pick.shape, \"ts_drop shape:\", ts_drop.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47a51378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ts_pick_feat cols: ['lag_1', 'lag_24', 'lag_168', 'rmean_3h', 'rmean_24h', 'hour', 'weekday', 'gmm20_cluster']\n",
      "[INFO] ts_drop_feat cols: ['lag_1', 'lag_24', 'lag_168', 'rmean_3h', 'rmean_24h', 'hour', 'weekday', 'gmm20_cluster']\n",
      "[INFO] ts_pick_feat shape: (4560, 11)\n",
      "[INFO] ts_drop_feat shape: (4560, 11)\n"
     ]
    }
   ],
   "source": [
    "def add_lags_rolls(df_in, value_col, lags=(1,24,168)):\n",
    "    df = df_in.sort_values([\"gmm20_cluster\",\"timestamp\"]).copy()\n",
    "\n",
    "    for lag in lags:\n",
    "        df[f\"lag_{lag}\"] = df.groupby(\"gmm20_cluster\")[value_col].shift(lag)\n",
    "\n",
    "    shifted = df.groupby(\"gmm20_cluster\")[value_col].shift(1)\n",
    "    df[\"rmean_3h\"]  = shifted.groupby(df[\"gmm20_cluster\"]).rolling(3).mean().reset_index(level=0, drop=True)\n",
    "    df[\"rmean_24h\"] = shifted.groupby(df[\"gmm20_cluster\"]).rolling(24).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "    df[\"hour\"] = df[\"timestamp\"].dt.hour.astype(\"int8\")\n",
    "    df[\"weekday\"] = df[\"timestamp\"].dt.weekday.astype(\"int8\")\n",
    "\n",
    "    # This is your target in training\n",
    "    df[\"y\"] = df[value_col].astype(float)\n",
    "    return df\n",
    "\n",
    "ts_pick_feat = add_lags_rolls(ts_pick, \"pickups\", lags=(1,24,168)).dropna().reset_index(drop=True)\n",
    "ts_drop_feat = add_lags_rolls(ts_drop, \"dropoffs\", lags=(1,24,168)).dropna().reset_index(drop=True)\n",
    "\n",
    "print(\"[INFO] ts_pick_feat cols:\", [c for c in FEATURES if c in ts_pick_feat.columns])\n",
    "print(\"[INFO] ts_drop_feat cols:\", [c for c in FEATURES if c in ts_drop_feat.columns])\n",
    "print(\"[INFO] ts_pick_feat shape:\", ts_pick_feat.shape)\n",
    "print(\"[INFO] ts_drop_feat shape:\", ts_drop_feat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db1ad1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Sanity passed: features + October coverage.\n",
      "[INFO] October rows pickups: 1488 dropoffs: 1488\n"
     ]
    }
   ],
   "source": [
    "missing_pick = [c for c in FEATURES if c not in ts_pick_feat.columns]\n",
    "missing_drop = [c for c in FEATURES if c not in ts_drop_feat.columns]\n",
    "if missing_pick:\n",
    "    raise ValueError(f\"Pickups feature table missing columns: {missing_pick}\")\n",
    "if missing_drop:\n",
    "    raise ValueError(f\"Dropoffs feature table missing columns: {missing_drop}\")\n",
    "\n",
    "oct_pick = ts_pick_feat[(ts_pick_feat[\"timestamp\"] >= VAL_START) & (ts_pick_feat[\"timestamp\"] < VAL_END)]\n",
    "oct_drop = ts_drop_feat[(ts_drop_feat[\"timestamp\"] >= VAL_START) & (ts_drop_feat[\"timestamp\"] < VAL_END)]\n",
    "if len(oct_pick) == 0 or len(oct_drop) == 0:\n",
    "    raise ValueError(\"No October rows found after feature creation. Check time window / buffer.\")\n",
    "\n",
    "print(\"[OK] Sanity passed: features + October coverage.\")\n",
    "print(\"[INFO] October rows pickups:\", len(oct_pick), \"dropoffs:\", len(oct_drop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec3fed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Cluster 0 | Oct MAE mean=12.332591 | saved preds_regressor/reg_cluster_0_preds.parquet\n",
      "[OK] Cluster 1 | Oct MAE mean=19.066918 | saved preds_regressor/reg_cluster_1_preds.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>val_mae_pickups</th>\n",
       "      <th>val_mae_dropoffs</th>\n",
       "      <th>val_mae_mean</th>\n",
       "      <th>parquet_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12.110037</td>\n",
       "      <td>12.555144</td>\n",
       "      <td>12.332591</td>\n",
       "      <td>preds_regressor/reg_cluster_0_preds.parquet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.278767</td>\n",
       "      <td>17.855070</td>\n",
       "      <td>19.066918</td>\n",
       "      <td>preds_regressor/reg_cluster_1_preds.parquet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster_id  val_mae_pickups  val_mae_dropoffs  val_mae_mean  \\\n",
       "0           0        12.110037         12.555144     12.332591   \n",
       "1           1        20.278767         17.855070     19.066918   \n",
       "\n",
       "                                  parquet_path  \n",
       "0  preds_regressor/reg_cluster_0_preds.parquet  \n",
       "1  preds_regressor/reg_cluster_1_preds.parquet  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_block(ts):\n",
    "    val = ts[(ts[\"timestamp\"] >= VAL_START) & (ts[\"timestamp\"] < VAL_END)].copy()\n",
    "    test = ts[ts[\"timestamp\"] >= TEST_START].copy()\n",
    "    return val, test\n",
    "\n",
    "def to_output_df(cluster_id, split_name, pick_block, drop_block, pred_pick, pred_drop):\n",
    "    # inner join on timestamp/hour for safety\n",
    "    a = pick_block[[\"timestamp\",\"hour\",\"y\"]].rename(columns={\"y\":\"y_true_pickups\"})\n",
    "    b = drop_block[[\"timestamp\",\"hour\",\"y\"]].rename(columns={\"y\":\"y_true_dropoffs\"})\n",
    "    out = a.merge(b, on=[\"timestamp\",\"hour\"], how=\"inner\")\n",
    "\n",
    "    out[\"y_pred_reg_pickups\"] = pred_pick[:len(out)]\n",
    "    out[\"y_pred_reg_dropoffs\"] = pred_drop[:len(out)]\n",
    "\n",
    "    out[\"ae_pickups_reg\"] = (out[\"y_true_pickups\"] - out[\"y_pred_reg_pickups\"]).abs()\n",
    "    out[\"ae_dropoffs_reg\"] = (out[\"y_true_dropoffs\"] - out[\"y_pred_reg_dropoffs\"]).abs()\n",
    "    out[\"ae_mean_reg\"] = 0.5 * (out[\"ae_pickups_reg\"] + out[\"ae_dropoffs_reg\"])\n",
    "\n",
    "    out[\"date\"] = pd.to_datetime(out[\"timestamp\"]).dt.normalize()\n",
    "    out[\"cluster_id\"] = cluster_id\n",
    "    out[\"split\"] = split_name\n",
    "\n",
    "    return out[[\n",
    "        \"date\",\"hour\",\"cluster_id\",\"split\",\n",
    "        \"y_true_pickups\",\"y_pred_reg_pickups\",\n",
    "        \"y_true_dropoffs\",\"y_pred_reg_dropoffs\",\n",
    "        \"ae_pickups_reg\",\"ae_dropoffs_reg\",\"ae_mean_reg\"\n",
    "    ]]\n",
    "\n",
    "summaries = []\n",
    "\n",
    "for cid in CLUSTERS_TO_MODEL:\n",
    "    pick_c = ts_pick_feat[ts_pick_feat[\"gmm20_cluster\"] == cid].copy()\n",
    "    drop_c = ts_drop_feat[ts_drop_feat[\"gmm20_cluster\"] == cid].copy()\n",
    "\n",
    "    val_p, test_p = split_block(pick_c)\n",
    "    val_d, test_d = split_block(drop_c)\n",
    "\n",
    "    # --- October predictions (for MAE / weight) ---\n",
    "    pred_val_p = dep_model.predict(val_p[FEATURES])\n",
    "    pred_val_d = arr_model.predict(val_d[FEATURES])\n",
    "\n",
    "    mae_val_pickups  = mean_absolute_error(val_p[\"y\"].values, pred_val_p)\n",
    "    mae_val_dropoffs = mean_absolute_error(val_d[\"y\"].values, pred_val_d)\n",
    "    mae_val_mean = 0.5 * (mae_val_pickups + mae_val_dropoffs)\n",
    "\n",
    "    df_val_out = to_output_df(cid, \"val\", val_p, val_d, pred_val_p, pred_val_d)\n",
    "\n",
    "    # --- Novâ€“Dec predictions ---\n",
    "    if len(test_p) > 0 and len(test_d) > 0:\n",
    "        pred_test_p = dep_model.predict(test_p[FEATURES])\n",
    "        pred_test_d = arr_model.predict(test_d[FEATURES])\n",
    "        df_test_out = to_output_df(cid, \"test\", test_p, test_d, pred_test_p, pred_test_d)\n",
    "        df_out = pd.concat([df_val_out, df_test_out], ignore_index=True)\n",
    "    else:\n",
    "        df_out = df_val_out.copy()\n",
    "\n",
    "    parquet_path = PRED_DIR / f\"reg_cluster_{cid}_preds.parquet\"\n",
    "    df_out.to_parquet(parquet_path, index=False)\n",
    "\n",
    "    summaries.append({\n",
    "        \"cluster_id\": cid,\n",
    "        \"val_mae_pickups\": float(mae_val_pickups),\n",
    "        \"val_mae_dropoffs\": float(mae_val_dropoffs),\n",
    "        \"val_mae_mean\": float(mae_val_mean),\n",
    "        \"parquet_path\": str(parquet_path),\n",
    "    })\n",
    "\n",
    "    print(f\"[OK] Cluster {cid} | Oct MAE mean={mae_val_mean:.6f} | saved {parquet_path}\")\n",
    "\n",
    "df_summary = pd.DataFrame(summaries)\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0d5b3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] General October MAE (Regressor): 15.699755\n",
      "[INFO] Raw weight (inverse MAE):        0.063695\n",
      "[INFO] Saved: artifacts_regressor/reg_general_weight.json\n"
     ]
    }
   ],
   "source": [
    "if len(df_summary) == 0:\n",
    "    raise ValueError(\"No clusters processed; cannot compute general MAE/weight.\")\n",
    "\n",
    "general_mae_reg = float(df_summary[\"val_mae_mean\"].mean())\n",
    "eps = 1e-6\n",
    "general_weight_reg = 1.0 / (general_mae_reg + eps)  # raw; normalize later across models\n",
    "\n",
    "info = {\n",
    "    \"model\": \"Regressor\",\n",
    "    \"clusters\": CLUSTERS_TO_MODEL,\n",
    "    \"general_mae_val_oct\": general_mae_reg,\n",
    "    \"general_weight_raw\": general_weight_reg,\n",
    "    \"val_start\": str(VAL_START.date()),\n",
    "    \"val_end\": str(VAL_END.date()),\n",
    "    \"features\": FEATURES,\n",
    "    \"num_feats\": NUM_FEATS,\n",
    "    \"cat_feats\": CAT_FEATS,\n",
    "    \"history_buffer_days\": HISTORY_BUFFER_DAYS,\n",
    "    \"dep_model_path\": str(DEP_MODEL_PATH),\n",
    "    \"arr_model_path\": str(ARR_MODEL_PATH),\n",
    "}\n",
    "\n",
    "(ARTIFACTS_DIR / \"reg_general_weight.json\").write_text(json.dumps(info, indent=2))\n",
    "joblib.dump(dep_model, ARTIFACTS_DIR / \"regressor_departures.joblib\")\n",
    "joblib.dump(arr_model, ARTIFACTS_DIR / \"regressor_arrivals.joblib\")\n",
    "\n",
    "print(f\"[INFO] General October MAE (Regressor): {general_mae_reg:.6f}\")\n",
    "print(f\"[INFO] Raw weight (inverse MAE):        {general_weight_reg:.6f}\")\n",
    "print(f\"[INFO] Saved: {ARTIFACTS_DIR / 'reg_general_weight.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b01ded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

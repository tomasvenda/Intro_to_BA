{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787f42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Inputs\n",
    "MODEL_PATH = Path(\"random_forrest_model.joblib\")\n",
    "DATA_PATH = Path(\"data_forrest.parquet\")\n",
    "\n",
    "CLUSTERS_TO_MODEL = [0, 1]\n",
    "\n",
    "VAL_START = pd.to_datetime(\"2018-10-01\")\n",
    "VAL_END   = pd.to_datetime(\"2018-11-01\")  # exclusive\n",
    "TEST_START = pd.to_datetime(\"2018-11-01\")\n",
    "\n",
    "# Features must match training\n",
    "FEATURES = [\n",
    "    \"cluster_id\", \"hour\", \"day_of_week\", \"month\",\n",
    "    \"is_weekend\", \"is_holiday\", \"lag_24h\", \"lag_168h\"\n",
    "]\n",
    "\n",
    "# Outputs\n",
    "ARTIFACTS_DIR = Path(\"artifacts_rf\")\n",
    "PRED_DIR = Path(\"preds_rf\")\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PRED_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "152e762e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomasvenda/miniconda3/envs/deep_learning/lib/python3.11/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/tomasvenda/miniconda3/envs/deep_learning/lib/python3.11/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.5.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "assert MODEL_PATH.exists(), f\"Missing model: {MODEL_PATH}\"\n",
    "assert DATA_PATH.exists(), f\"Missing data: {DATA_PATH}\"\n",
    "\n",
    "loaded = joblib.load(MODEL_PATH)\n",
    "rf_pickups = loaded.get(\"pickups\", None)\n",
    "rf_dropoffs = loaded.get(\"dropoffs\", None)  # may be None\n",
    "\n",
    "if rf_pickups is None:\n",
    "    raise ValueError(\"random_forrest_model.joblib does not contain key 'pickups'.\")\n",
    "\n",
    "df = pd.read_parquet(DATA_PATH).copy()\n",
    "\n",
    "# Expect either:\n",
    "# - datetime column (hourly timestamp), OR separate date/hour\n",
    "if \"datetime\" not in df.columns:\n",
    "    raise ValueError(\"data_forrest.parquet must contain a 'datetime' column.\")\n",
    "\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "df[\"date\"] = df[\"datetime\"].dt.normalize()\n",
    "df[\"hour\"] = df[\"datetime\"].dt.hour  # ensure hour exists/consistent\n",
    "\n",
    "missing = [c for c in FEATURES if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required feature columns: {missing}\")\n",
    "\n",
    "# Ground truth columns needed:\n",
    "# pickups is required; dropoffs if you want arrivals too\n",
    "if \"pickups\" not in df.columns:\n",
    "    raise ValueError(\"data_forrest.parquet must contain ground truth column 'pickups'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "391e3301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Exported cluster 0\n",
      "[OK] Exported cluster 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>val_mae_pickups</th>\n",
       "      <th>val_mae_dropoffs</th>\n",
       "      <th>val_mae_mean</th>\n",
       "      <th>parquet_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6.126288</td>\n",
       "      <td>6.256721</td>\n",
       "      <td>6.191504</td>\n",
       "      <td>preds_rf/rf_cluster_0_preds.parquet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>16.897902</td>\n",
       "      <td>18.168927</td>\n",
       "      <td>17.533415</td>\n",
       "      <td>preds_rf/rf_cluster_1_preds.parquet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster_id  val_mae_pickups  val_mae_dropoffs  val_mae_mean  \\\n",
       "0           0         6.126288          6.256721      6.191504   \n",
       "1           1        16.897902         18.168927     17.533415   \n",
       "\n",
       "                          parquet_path  \n",
       "0  preds_rf/rf_cluster_0_preds.parquet  \n",
       "1  preds_rf/rf_cluster_1_preds.parquet  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_split(df_):\n",
    "    val = df_[(df_[\"datetime\"] >= VAL_START) & (df_[\"datetime\"] < VAL_END)].copy()\n",
    "    test = df_[df_[\"datetime\"] >= TEST_START].copy()\n",
    "    return val, test\n",
    "\n",
    "def predict_block(model, block: pd.DataFrame, target_name: str, pred_col: str):\n",
    "    X = block[FEATURES]\n",
    "    block[pred_col] = model.predict(X)\n",
    "    # absolute error if ground truth exists\n",
    "    if target_name in block.columns:\n",
    "        block[f\"ae_{pred_col}\"] = (block[target_name] - block[pred_col]).abs()\n",
    "    return block\n",
    "\n",
    "def export_cluster(df, cluster_id: int):\n",
    "    dfi = df[df[\"cluster_id\"] == cluster_id].copy()\n",
    "    val, test = make_split(dfi)\n",
    "\n",
    "    if len(val) == 0:\n",
    "        raise ValueError(f\"Cluster {cluster_id}: empty October validation slice.\")\n",
    "    if len(test) == 0:\n",
    "        print(f\"[WARN] Cluster {cluster_id}: empty test slice (Nov-Dec).\")\n",
    "\n",
    "    # --- VAL predictions (for MAE/weight) ---\n",
    "    val = predict_block(rf_pickups, val, \"pickups\", \"y_pred_rf_pickups\")\n",
    "\n",
    "    mae_val_pickups = mean_absolute_error(val[\"pickups\"], val[\"y_pred_rf_pickups\"])\n",
    "\n",
    "    # dropoffs optional\n",
    "    mae_val_dropoffs = None\n",
    "    if rf_dropoffs is not None and \"dropoffs\" in val.columns:\n",
    "        val = predict_block(rf_dropoffs, val, \"dropoffs\", \"y_pred_rf_dropoffs\")\n",
    "        mae_val_dropoffs = mean_absolute_error(val[\"dropoffs\"], val[\"y_pred_rf_dropoffs\"])\n",
    "\n",
    "    # --- TEST predictions ---\n",
    "    if len(test) > 0:\n",
    "        test = predict_block(rf_pickups, test, \"pickups\", \"y_pred_rf_pickups\")\n",
    "\n",
    "        if rf_dropoffs is not None and \"dropoffs\" in test.columns:\n",
    "            test = predict_block(rf_dropoffs, test, \"dropoffs\", \"y_pred_rf_dropoffs\")\n",
    "\n",
    "    # Standardize output columns to match MLP parquet schema as closely as possible\n",
    "    def finalize(block, split_name):\n",
    "        out = pd.DataFrame({\n",
    "            \"date\": block[\"date\"].values,\n",
    "            \"hour\": block[\"hour\"].values,\n",
    "            \"cluster_id\": block[\"cluster_id\"].values,\n",
    "            \"split\": split_name,\n",
    "            \"y_true_pickups\": block[\"pickups\"].values,\n",
    "            \"y_pred_rf_pickups\": block[\"y_pred_rf_pickups\"].values,\n",
    "        })\n",
    "        out[\"ae_pickups_rf\"] = (out[\"y_true_pickups\"] - out[\"y_pred_rf_pickups\"]).abs()\n",
    "\n",
    "        # dropoffs if available\n",
    "        if \"dropoffs\" in block.columns and \"y_pred_rf_dropoffs\" in block.columns:\n",
    "            out[\"y_true_dropoffs\"] = block[\"dropoffs\"].values\n",
    "            out[\"y_pred_rf_dropoffs\"] = block[\"y_pred_rf_dropoffs\"].values\n",
    "            out[\"ae_dropoffs_rf\"] = (out[\"y_true_dropoffs\"] - out[\"y_pred_rf_dropoffs\"]).abs()\n",
    "            out[\"ae_mean_rf\"] = 0.5 * (out[\"ae_pickups_rf\"] + out[\"ae_dropoffs_rf\"])\n",
    "        else:\n",
    "            out[\"y_true_dropoffs\"] = np.nan\n",
    "            out[\"y_pred_rf_dropoffs\"] = np.nan\n",
    "            out[\"ae_dropoffs_rf\"] = np.nan\n",
    "            out[\"ae_mean_rf\"] = out[\"ae_pickups_rf\"]  # fallback if only pickups\n",
    "        return out\n",
    "\n",
    "    df_val_out = finalize(val, \"val\")\n",
    "    df_test_out = finalize(test, \"test\") if len(test) > 0 else pd.DataFrame(columns=df_val_out.columns)\n",
    "\n",
    "    df_out = pd.concat([df_val_out, df_test_out], ignore_index=True)\n",
    "\n",
    "    parquet_path = PRED_DIR / f\"rf_cluster_{cluster_id}_preds.parquet\"\n",
    "    df_out.to_parquet(parquet_path, index=False)\n",
    "\n",
    "    summary = {\n",
    "        \"cluster_id\": cluster_id,\n",
    "        \"val_mae_pickups\": float(mae_val_pickups),\n",
    "        \"val_mae_dropoffs\": None if mae_val_dropoffs is None else float(mae_val_dropoffs),\n",
    "        \"val_mae_mean\": float(df_val_out[\"ae_mean_rf\"].mean()),\n",
    "        \"parquet_path\": str(parquet_path),\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "summaries = []\n",
    "for cid in CLUSTERS_TO_MODEL:\n",
    "    try:\n",
    "        summaries.append(export_cluster(df, cid))\n",
    "        print(f\"[OK] Exported cluster {cid}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"[WARN] {e}\")\n",
    "\n",
    "df_summary = pd.DataFrame(summaries)\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72159ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] RF general Oct MAE: 11.862459\n",
      "[INFO] RF raw weight: 0.084300\n",
      "[INFO] Saved: artifacts_rf/rf_general_weight.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['artifacts_rf/random_forrest_model.joblib']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(df_summary) == 0:\n",
    "    raise ValueError(\"No clusters exported; cannot compute general MAE/weight.\")\n",
    "\n",
    "general_mae_rf = float(df_summary[\"val_mae_mean\"].mean())\n",
    "eps = 1e-6\n",
    "general_weight_rf = 1.0 / (general_mae_rf + eps)  # raw weight; normalize later across models\n",
    "\n",
    "weight_info = {\n",
    "    \"model\": \"RandomForest\",\n",
    "    \"clusters\": CLUSTERS_TO_MODEL,\n",
    "    \"general_mae_val_oct\": general_mae_rf,\n",
    "    \"general_weight_raw\": general_weight_rf,\n",
    "    \"val_start\": str(VAL_START.date()),\n",
    "    \"val_end\": str(VAL_END.date()),\n",
    "    \"features\": FEATURES,\n",
    "    \"has_dropoffs_model\": rf_dropoffs is not None,\n",
    "}\n",
    "\n",
    "weights_path = ARTIFACTS_DIR / \"rf_general_weight.json\"\n",
    "weights_path.write_text(json.dumps(weight_info, indent=2))\n",
    "print(f\"[INFO] RF general Oct MAE: {general_mae_rf:.6f}\")\n",
    "print(f\"[INFO] RF raw weight: {general_weight_rf:.6f}\")\n",
    "print(f\"[INFO] Saved: {weights_path}\")\n",
    "\n",
    "# Optional: copy model artifact into artifacts_rf for consistency\n",
    "joblib.dump(loaded, ARTIFACTS_DIR / \"random_forrest_model.joblib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

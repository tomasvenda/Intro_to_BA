{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c5031fc",
   "metadata": {},
   "source": [
    "### 1. Setup and Configuration\n",
    "Imports libraries, defines file paths, and sets time windows for validation (Nov) and testing (Dec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb62db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Config set.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "MODEL_PATH = Path(\"Data/random_forrest_model.joblib\")\n",
    "DATA_PATH = Path(\"Data/data_forrest.parquet\")\n",
    "\n",
    "CLUSTERS_TO_MODEL = [0, 8]\n",
    "\n",
    "VAL_START  = pd.Timestamp(\"2018-11-01 00:00:00\")\n",
    "VAL_END    = pd.Timestamp(\"2018-12-01 00:00:00\")  # exclusive\n",
    "TEST_START = pd.Timestamp(\"2018-12-01 00:00:00\")\n",
    "TEST_END   = pd.Timestamp(\"2018-12-31 23:00:00\")\n",
    "\n",
    "# Features must match training (but we'll recompute lag_24h / lag_168h ourselves)\n",
    "FEATURES = [\n",
    "    \"cluster_id\", \"hour\", \"day_of_week\", \"month\",\n",
    "    \"is_weekend\", \"is_holiday\", \"lag_24h\", \"lag_168h\"\n",
    "]\n",
    "\n",
    "ARTIFACTS_DIR = Path(\"artifacts_rf\")\n",
    "PRED_DIR = Path(\"preds_rf\")\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EPS = 1e-6\n",
    "print(\"[OK] Config set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9a8f6",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Preprocessing\n",
    "Loads the pre-trained Random Forest model and raw data, filtering for specific clusters and ensuring sufficient history exists for lag generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c398a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] dropoffs truth present: True\n",
      "[INFO] df rows: 171840 clusters: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[INFO] filtered df rows: 3264 from 2018-10-25 00:00:00 to 2018-12-31 23:00:00\n"
     ]
    }
   ],
   "source": [
    "assert MODEL_PATH.exists(), f\"Missing model: {MODEL_PATH}\"\n",
    "assert DATA_PATH.exists(), f\"Missing data: {DATA_PATH}\"\n",
    "\n",
    "loaded = joblib.load(MODEL_PATH)\n",
    "rf_pickups = loaded.get(\"pickups\", None)\n",
    "rf_dropoffs = loaded.get(\"dropoffs\", None)  # may be None\n",
    "\n",
    "if rf_pickups is None:\n",
    "    raise ValueError(\"random_forrest_model.joblib does not contain key 'pickups'.\")\n",
    "\n",
    "df = pd.read_parquet(DATA_PATH).copy()\n",
    "\n",
    "if \"datetime\" not in df.columns:\n",
    "    raise ValueError(\"data_forrest.parquet must contain a 'datetime' column.\")\n",
    "\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"]).dt.floor(\"h\")\n",
    "df[\"date\"] = df[\"datetime\"].dt.normalize()\n",
    "df[\"hour\"] = df[\"datetime\"].dt.hour.astype(int)\n",
    "\n",
    "# Must contain cluster_id + ground truth pickups\n",
    "need_cols = {\"cluster_id\", \"datetime\", \"pickups\"}\n",
    "missing = need_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "# Optional dropoffs\n",
    "has_dropoffs_truth = \"dropoffs\" in df.columns\n",
    "print(\"[INFO] dropoffs truth present:\", has_dropoffs_truth)\n",
    "print(\"[INFO] df rows:\", len(df), \"clusters:\", sorted(df[\"cluster_id\"].unique())[:10])\n",
    "\n",
    "# Filter to modeled clusters and to required time span incl. needed history for lags\n",
    "HIST_START = (VAL_START - pd.Timedelta(hours=168)).floor(\"h\")\n",
    "df = df[(df[\"cluster_id\"].isin(CLUSTERS_TO_MODEL)) &\n",
    "        (df[\"datetime\"] >= HIST_START) &\n",
    "        (df[\"datetime\"] <= TEST_END)].copy()\n",
    "\n",
    "print(\"[INFO] filtered df rows:\", len(df), \"from\", df[\"datetime\"].min(), \"to\", df[\"datetime\"].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0043a44",
   "metadata": {},
   "source": [
    "### 3. Feature Engineering\n",
    "Reconstructs the hourly time grid to handle missing data, computes calendar features, and generates lag features (24h, 168h) dynamically to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b6fdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] base_pred rows after lag drop: 2928\n",
      "[INFO] time span base_pred: 2018-11-01 00:00:00 to 2018-12-31 23:00:00\n"
     ]
    }
   ],
   "source": [
    "def make_hour_grid(clusters, start, end):\n",
    "    hours = pd.date_range(start, end, freq=\"h\")\n",
    "    grid = pd.MultiIndex.from_product([clusters, hours], names=[\"cluster_id\", \"datetime\"]).to_frame(index=False)\n",
    "    return grid\n",
    "\n",
    "grid = make_hour_grid(CLUSTERS_TO_MODEL, HIST_START, TEST_END)\n",
    "\n",
    "# Merge & fill missing pickups/dropoffs with 0\n",
    "base = grid.merge(df[[\"cluster_id\",\"datetime\",\"pickups\"] + ([\"dropoffs\"] if has_dropoffs_truth else [])],\n",
    "                  on=[\"cluster_id\",\"datetime\"], how=\"left\")\n",
    "\n",
    "base[\"pickups\"] = base[\"pickups\"].fillna(0).astype(float)\n",
    "if has_dropoffs_truth:\n",
    "    base[\"dropoffs\"] = base[\"dropoffs\"].fillna(0).astype(float)\n",
    "\n",
    "# Recreate calendar features (donâ€™t trust parquet)\n",
    "base[\"hour\"] = base[\"datetime\"].dt.hour.astype(int)\n",
    "base[\"day_of_week\"] = base[\"datetime\"].dt.dayofweek.astype(int)\n",
    "base[\"month\"] = base[\"datetime\"].dt.month.astype(int)\n",
    "base[\"is_weekend\"] = (base[\"day_of_week\"] >= 5).astype(int)\n",
    "\n",
    "# IMPORTANT: keep is_holiday from parquet if it exists, otherwise default to 0\n",
    "if \"is_holiday\" in df.columns:\n",
    "    hol = df[[\"cluster_id\",\"datetime\",\"is_holiday\"]].copy()\n",
    "    hol[\"datetime\"] = pd.to_datetime(hol[\"datetime\"]).dt.floor(\"h\")\n",
    "    base = base.merge(hol, on=[\"cluster_id\",\"datetime\"], how=\"left\")\n",
    "    base[\"is_holiday\"] = base[\"is_holiday\"].fillna(0).astype(int)\n",
    "else:\n",
    "    base[\"is_holiday\"] = 0\n",
    "\n",
    "# Leak-free lags computed from the true past series (allowed at prediction time)\n",
    "base = base.sort_values([\"cluster_id\",\"datetime\"]).reset_index(drop=True)\n",
    "base[\"lag_24h\"]  = base.groupby(\"cluster_id\")[\"pickups\"].shift(24)\n",
    "base[\"lag_168h\"] = base.groupby(\"cluster_id\")[\"pickups\"].shift(168)\n",
    "\n",
    "# For dropoffs model, compute dropoff lags too (only if predicting dropoffs)\n",
    "if rf_dropoffs is not None and has_dropoffs_truth:\n",
    "    base[\"lag_24h_drop\"]  = base.groupby(\"cluster_id\")[\"dropoffs\"].shift(24)\n",
    "    base[\"lag_168h_drop\"] = base.groupby(\"cluster_id\")[\"dropoffs\"].shift(168)\n",
    "\n",
    "# Drop rows where pickups lags are not available (first 168h)\n",
    "base_pred = base.dropna(subset=[\"lag_24h\",\"lag_168h\"]).copy()\n",
    "\n",
    "print(\"[INFO] base_pred rows after lag drop:\", len(base_pred))\n",
    "print(\"[INFO] time span base_pred:\", base_pred[\"datetime\"].min(), \"to\", base_pred[\"datetime\"].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb42af1",
   "metadata": {},
   "source": [
    "### 4. Prediction and Evaluation Loop\n",
    "Iterates through clusters to generate predictions for pickups and dropoffs, computes RMSE for the validation set, and saves results to parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b8c3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Cluster 0 | VAL rmse_pick=13.361 | saved preds_rf\\rf_cluster_0_preds.parquet\n",
      "[OK] Cluster 8 | VAL rmse_pick=120.064 | saved preds_rf\\rf_cluster_8_preds.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>val_rmse_pickups</th>\n",
       "      <th>val_rmse_dropoffs</th>\n",
       "      <th>val_rmse_mean</th>\n",
       "      <th>parquet_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.360802</td>\n",
       "      <td>16.924097</td>\n",
       "      <td>15.142450</td>\n",
       "      <td>preds_rf\\rf_cluster_0_preds.parquet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>120.063918</td>\n",
       "      <td>165.202078</td>\n",
       "      <td>142.632998</td>\n",
       "      <td>preds_rf\\rf_cluster_8_preds.parquet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster_id  val_rmse_pickups  val_rmse_dropoffs  val_rmse_mean  \\\n",
       "0           0         13.360802          16.924097      15.142450   \n",
       "1           8        120.063918         165.202078     142.632998   \n",
       "\n",
       "                          parquet_path  \n",
       "0  preds_rf\\rf_cluster_0_preds.parquet  \n",
       "1  preds_rf\\rf_cluster_8_preds.parquet  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_split(df_):\n",
    "    val = df_[(df_[\"datetime\"] >= VAL_START) & (df_[\"datetime\"] < VAL_END)].copy()\n",
    "    test = df_[(df_[\"datetime\"] >= TEST_START) & (df_[\"datetime\"] <= TEST_END)].copy()\n",
    "    return val, test\n",
    "\n",
    "def predict_pickups(block: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = block[FEATURES]\n",
    "    block[\"y_pred_rf_pickups\"] = rf_pickups.predict(X)\n",
    "    return block\n",
    "\n",
    "def predict_dropoffs(block: pd.DataFrame) -> pd.DataFrame:\n",
    "    # The RF model was trained on generic feature names 'lag_24h', 'lag_168h'.\n",
    "    # When predicting dropoffs, we must feed the *dropoff* lags into those specific column names\n",
    "    # so the model sees the correct history.\n",
    "    block[\"lag_24h\"] = block[\"lag_24h_drop\"]\n",
    "    block[\"lag_168h\"] = block[\"lag_168h_drop\"]\n",
    "    X = block[FEATURES]\n",
    "    block[\"y_pred_rf_dropoffs\"] = rf_dropoffs.predict(X)\n",
    "    return block\n",
    "\n",
    "def finalize(block: pd.DataFrame, split_name: str) -> pd.DataFrame:\n",
    "    out = pd.DataFrame({\n",
    "        \"date\": block[\"datetime\"].dt.normalize(),\n",
    "        \"hour\": block[\"datetime\"].dt.hour.astype(int),\n",
    "        \"cluster_id\": block[\"cluster_id\"].astype(int),\n",
    "        \"split\": split_name,\n",
    "        \"y_true_pickups\": block[\"pickups\"].astype(float),\n",
    "        \"y_pred_rf_pickups\": block[\"y_pred_rf_pickups\"].astype(float),\n",
    "    })\n",
    "    out[\"se_pickups_rf\"] = (out[\"y_true_pickups\"] - out[\"y_pred_rf_pickups\"])**2\n",
    "\n",
    "    if rf_dropoffs is not None and has_dropoffs_truth and \"y_pred_rf_dropoffs\" in block.columns:\n",
    "        out[\"y_true_dropoffs\"] = block[\"dropoffs\"].astype(float)\n",
    "        out[\"y_pred_rf_dropoffs\"] = block[\"y_pred_rf_dropoffs\"].astype(float)\n",
    "        out[\"se_dropoffs_rf\"] = (out[\"y_true_dropoffs\"] - out[\"y_pred_rf_dropoffs\"])**2\n",
    "    else:\n",
    "        out[\"y_true_dropoffs\"] = np.nan\n",
    "        out[\"y_pred_rf_dropoffs\"] = np.nan\n",
    "        out[\"se_dropoffs_rf\"] = np.nan\n",
    "\n",
    "    return out\n",
    "\n",
    "summaries = []\n",
    "\n",
    "for cid in CLUSTERS_TO_MODEL:\n",
    "    dfi = base_pred[base_pred[\"cluster_id\"] == cid].copy()\n",
    "    val, test = make_split(dfi)\n",
    "\n",
    "    if len(val) == 0:\n",
    "        print(f\"[WARN] Cluster {cid}: empty validation slice.\")\n",
    "        continue\n",
    "\n",
    "    # pickups\n",
    "    val = predict_pickups(val)\n",
    "    test = predict_pickups(test) if len(test) else test\n",
    "\n",
    "    rmse_val_pickups = float(np.sqrt(mean_squared_error(val[\"pickups\"], val[\"y_pred_rf_pickups\"])))\n",
    "\n",
    "    # dropoffs model (optional, but handle leak-free by using dropoff lags)\n",
    "    rmse_val_dropoffs = None\n",
    "    if rf_dropoffs is not None and has_dropoffs_truth:\n",
    "        needed_drop_lags = {\"lag_24h_drop\",\"lag_168h_drop\"}\n",
    "        if not needed_drop_lags.issubset(val.columns):\n",
    "            raise ValueError(\"Dropoffs lags missing; cannot safely predict dropoffs.\")\n",
    "        val = predict_dropoffs(val)\n",
    "        if len(test):\n",
    "            test = predict_dropoffs(test)\n",
    "        rmse_val_dropoffs = float(np.sqrt(mean_squared_error(val[\"dropoffs\"], val[\"y_pred_rf_dropoffs\"])))\n",
    "\n",
    "    df_val_out = finalize(val, \"val\")\n",
    "    df_test_out = finalize(test, \"test\") if len(test) else pd.DataFrame(columns=df_val_out.columns)\n",
    "\n",
    "    out = pd.concat([df_val_out, df_test_out], ignore_index=True)\n",
    "    out_path = PRED_DIR / f\"rf_cluster_{cid}_preds.parquet\"\n",
    "    out.to_parquet(out_path, index=False)\n",
    "\n",
    "    # rmse_mean definition consistent with your ensemble code\n",
    "    rmse_mean_val = 0.5*(rmse_val_pickups + (rmse_val_dropoffs if rmse_val_dropoffs is not None else rmse_val_pickups))\n",
    "\n",
    "    summaries.append({\n",
    "        \"cluster_id\": cid,\n",
    "        \"val_rmse_pickups\": rmse_val_pickups,\n",
    "        \"val_rmse_dropoffs\": rmse_val_dropoffs,\n",
    "        \"val_rmse_mean\": rmse_mean_val,\n",
    "        \"parquet_path\": str(out_path),\n",
    "    })\n",
    "\n",
    "    print(f\"[OK] Cluster {cid} | VAL rmse_pick={rmse_val_pickups:.3f} | saved {out_path}\")\n",
    "\n",
    "df_summary = pd.DataFrame(summaries)\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d4561",
   "metadata": {},
   "source": [
    "### 5. Global Weighting and Artifacts\n",
    "Aggregates performance metrics across clusters to calculate a global ensemble weight and saves the final configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47831d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] RF general VAL RMSE mean: 78.887724\n",
      "[INFO] RF raw weight: 0.012676\n",
      "[INFO] Saved: artifacts_rf\\rf_general_weight.json\n",
      "[INFO] Copied model to artifacts_rf/random_forrest_model.joblib\n"
     ]
    }
   ],
   "source": [
    "if len(df_summary) == 0:\n",
    "    raise ValueError(\"No clusters exported; cannot compute general RMSE/weight.\")\n",
    "\n",
    "general_rmse_rf = float(df_summary[\"val_rmse_mean\"].mean())\n",
    "general_weight_rf = 1.0 / (general_rmse_rf + EPS)\n",
    "\n",
    "weight_info = {\n",
    "    \"model\": \"RandomForest\",\n",
    "    \"clusters\": list(map(int, CLUSTERS_TO_MODEL)),\n",
    "    \"general_rmse_val_nov\": general_rmse_rf,\n",
    "    \"general_weight_raw\": general_weight_rf,\n",
    "    \"val_start\": str(VAL_START),\n",
    "    \"val_end\": str(VAL_END),\n",
    "    \"test_start\": str(TEST_START),\n",
    "    \"features\": FEATURES,\n",
    "    \"lags_recomputed_safely\": True,\n",
    "    \"has_dropoffs_model\": rf_dropoffs is not None,\n",
    "}\n",
    "\n",
    "weights_path = ARTIFACTS_DIR / \"rf_general_weight.json\"\n",
    "weights_path.write_text(json.dumps(weight_info, indent=2))\n",
    "\n",
    "print(f\"[INFO] RF general VAL RMSE mean: {general_rmse_rf:.6f}\")\n",
    "print(f\"[INFO] RF raw weight: {general_weight_rf:.6f}\")\n",
    "print(f\"[INFO] Saved: {weights_path}\")\n",
    "\n",
    "# Optional: copy model artifact into artifacts_rf\n",
    "joblib.dump(loaded, ARTIFACTS_DIR / \"random_forrest_model.joblib\")\n",
    "print(\"[INFO] Copied model to artifacts_rf/random_forrest_model.joblib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introToML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

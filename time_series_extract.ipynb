{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72855321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Outputs -> /mnt/data/Desktop/Masters/Business_Analytics/Intro_to_BA/preds_ts_daily\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from prophet.serialize import model_from_json\n",
    "\n",
    "# ================== INPUTS ==================\n",
    "HOURLY_PATH = Path(\"cluster_hourly.parquet\")\n",
    "\n",
    "SARIMAX_DIR = Path(\"sarimax/sarimax_best\")\n",
    "PROPHET_DIR = Path(\"prophet_folder/prophet_best\")\n",
    "\n",
    "CLUSTERS = [0, 8]\n",
    "\n",
    "# We evaluate day-ahead forecasts for these months\n",
    "VAL_MONTH = 11           # November (use Nov as validation)\n",
    "TEST_MONTHS = [12]       # December (test)\n",
    "\n",
    "HARD_END = pd.Timestamp(\"2018-12-31 23:00:00\")\n",
    "\n",
    "# Regressors used in training scripts\n",
    "LAGS = [24, 168]  # hours\n",
    "USE_US_HOLIDAYS_SARIMAX = True\n",
    "\n",
    "# Prophet used weekend/weekday conditional seasonality + lag regressors\n",
    "USE_LAGS_PROPHET = True\n",
    "\n",
    "# ================== OUTPUTS ==================\n",
    "PRED_OUT_DIR = Path(\"preds_ts_daily\")\n",
    "ART_SARIMAX_DIR = Path(\"artifacts_sarimax_daily\")\n",
    "ART_PROPHET_DIR = Path(\"artifacts_prophet_daily\")\n",
    "PRED_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ART_SARIMAX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ART_PROPHET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[INFO] Outputs ->\", PRED_OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0feee6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded: (183960, 10)\n",
      "[INFO] Columns: ['cluster', 'datetime_hour', 'departures', 'arrivals', 'was_missing_departures', 'was_missing_arrivals', 'weekday', 'hour', 'month', 'year']\n",
      "[INFO] After filtering: (17520, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>datetime_hour</th>\n",
       "      <th>departures</th>\n",
       "      <th>arrivals</th>\n",
       "      <th>was_missing_departures</th>\n",
       "      <th>was_missing_arrivals</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster       datetime_hour  departures  arrivals  was_missing_departures  \\\n",
       "0        0 2018-01-01 00:00:00         3.0       1.0                   False   \n",
       "1        0 2018-01-01 01:00:00         2.0       4.0                   False   \n",
       "2        0 2018-01-01 02:00:00         3.0       2.0                   False   \n",
       "3        0 2018-01-01 03:00:00         4.0       7.0                   False   \n",
       "4        0 2018-01-01 04:00:00         0.0       1.0                   False   \n",
       "\n",
       "   was_missing_arrivals  weekday  hour  month  year  \n",
       "0                 False        0     0      1  2018  \n",
       "1                 False        0     1      1  2018  \n",
       "2                 False        0     2      1  2018  \n",
       "3                 False        0     3      1  2018  \n",
       "4                 False        0     4      1  2018  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(HOURLY_PATH).copy()\n",
    "print(\"[INFO] Loaded:\", df.shape)\n",
    "print(\"[INFO] Columns:\", list(df.columns))\n",
    "\n",
    "# cluster\n",
    "if \"cluster\" not in df.columns:\n",
    "    if \"cluster_id\" in df.columns:\n",
    "        df = df.rename(columns={\"cluster_id\":\"cluster\"})\n",
    "    elif \"gmm20_cluster\" in df.columns:\n",
    "        df = df.rename(columns={\"gmm20_cluster\":\"cluster\"})\n",
    "    else:\n",
    "        raise ValueError(\"Need cluster column: 'cluster' or 'cluster_id' or 'gmm20_cluster'.\")\n",
    "\n",
    "# datetime_hour\n",
    "if \"datetime_hour\" not in df.columns:\n",
    "    if \"timestamp\" in df.columns:\n",
    "        df = df.rename(columns={\"timestamp\":\"datetime_hour\"})\n",
    "    elif \"datetime\" in df.columns:\n",
    "        df = df.rename(columns={\"datetime\":\"datetime_hour\"})\n",
    "    elif {\"date\",\"hour\"}.issubset(df.columns):\n",
    "        df[\"datetime_hour\"] = pd.to_datetime(df[\"date\"]).dt.normalize() + pd.to_timedelta(df[\"hour\"], unit=\"h\")\n",
    "    else:\n",
    "        raise ValueError(\"Need time column: 'datetime_hour' or 'timestamp'/'datetime' or ('date','hour').\")\n",
    "\n",
    "df[\"datetime_hour\"] = pd.to_datetime(df[\"datetime_hour\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"datetime_hour\"]).copy()\n",
    "\n",
    "# departures/arrivals\n",
    "if \"departures\" not in df.columns:\n",
    "    if \"pickups\" in df.columns:\n",
    "        df = df.rename(columns={\"pickups\":\"departures\"})\n",
    "    else:\n",
    "        raise ValueError(\"Need departures column: 'departures' or 'pickups'.\")\n",
    "\n",
    "if \"arrivals\" not in df.columns:\n",
    "    if \"dropoffs\" in df.columns:\n",
    "        df = df.rename(columns={\"dropoffs\":\"arrivals\"})\n",
    "    else:\n",
    "        raise ValueError(\"Need arrivals column: 'arrivals' or 'dropoffs'.\")\n",
    "\n",
    "# filter\n",
    "df = df[df[\"cluster\"].isin(CLUSTERS)].copy()\n",
    "df = df[df[\"datetime_hour\"] <= HARD_END].copy()\n",
    "df = df.sort_values([\"cluster\",\"datetime_hour\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"[INFO] After filtering:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d7c9f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] cluster 0: departures hours=8760, arrivals hours=8760\n",
      "[INFO] cluster 8: departures hours=8760, arrivals hours=8760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_925039/2085566942.py:7: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  s = out.set_index(\"ds\")[col_y].asfreq(\"H\")\n",
      "/tmp/ipykernel_925039/2085566942.py:7: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  s = out.set_index(\"ds\")[col_y].asfreq(\"H\")\n",
      "/tmp/ipykernel_925039/2085566942.py:7: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  s = out.set_index(\"ds\")[col_y].asfreq(\"H\")\n",
      "/tmp/ipykernel_925039/2085566942.py:7: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  s = out.set_index(\"ds\")[col_y].asfreq(\"H\")\n"
     ]
    }
   ],
   "source": [
    "def make_dense_series(df_c: pd.DataFrame, col_y: str) -> pd.Series:\n",
    "    tmp = df_c[[\"datetime_hour\", col_y]].copy()\n",
    "    tmp = tmp.groupby(\"datetime_hour\", as_index=False)[col_y].sum().sort_values(\"datetime_hour\")\n",
    "    idx = pd.date_range(tmp[\"datetime_hour\"].min(), tmp[\"datetime_hour\"].max(), freq=\"h\")\n",
    "    out = pd.DataFrame({\"ds\": idx}).merge(tmp.rename(columns={\"datetime_hour\":\"ds\"}), on=\"ds\", how=\"left\")\n",
    "    out[col_y] = out[col_y].fillna(0.0).astype(float)\n",
    "    s = out.set_index(\"ds\")[col_y].asfreq(\"H\")\n",
    "    if s.isna().any():\n",
    "        raise ValueError(\"Dense series still has NaNs — unexpected.\")\n",
    "    return s\n",
    "\n",
    "series = {}\n",
    "for c in CLUSTERS:\n",
    "    df_c = df[df[\"cluster\"] == c].copy()\n",
    "    series[c] = {\n",
    "        \"departures\": make_dense_series(df_c, \"departures\"),\n",
    "        \"arrivals\": make_dense_series(df_c, \"arrivals\"),\n",
    "    }\n",
    "    print(f\"[INFO] cluster {c}: departures hours={len(series[c]['departures'])}, arrivals hours={len(series[c]['arrivals'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7d7d27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training cluster 0 SARIMAX (departures) + Prophet (arrivals)...\n",
      "[INFO] cluster 0: SARIMAX converged=True, spec={'order': (1, 1, 2), 'seasonal_order': (1, 0, 1, 24)}, scale=130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:20:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "23:21:01 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training cluster 8 SARIMAX (departures) + Prophet (arrivals)...\n",
      "[INFO] cluster 8: SARIMAX converged=True, spec={'order': (0, 1, 1), 'seasonal_order': (1, 0, 1, 24)}, scale=1.23e+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:26:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "23:26:24 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Trained and saved SARIMAX + Prophet models for all clusters.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 (LIGHTWEIGHT) — fast SARIMAX + stable Prophet (no collinear regressors)\n",
    "\n",
    "import warnings\n",
    "import pickle\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "from prophet import Prophet\n",
    "from prophet.serialize import model_to_json\n",
    "\n",
    "def us_holiday_indicator(idx: pd.DatetimeIndex) -> pd.Series:\n",
    "    cal = USFederalHolidayCalendar()\n",
    "    hols = cal.holidays(start=idx.min().floor(\"D\"), end=idx.max().ceil(\"D\"))\n",
    "    return idx.floor(\"D\").isin(hols).astype(int)\n",
    "\n",
    "def prophet_conditions(df_future: pd.DataFrame) -> pd.DataFrame:\n",
    "    dow = df_future[\"ds\"].dt.dayofweek\n",
    "    df_future[\"is_weekend\"] = (dow >= 5)\n",
    "    df_future[\"is_weekday\"] = (dow < 5)\n",
    "    return df_future\n",
    "\n",
    "def build_lag_regressors_full(y: pd.Series, lags=LAGS) -> pd.DataFrame:\n",
    "    exog = pd.DataFrame(index=y.index)\n",
    "    for L in lags:\n",
    "        exog[f\"y_lag_{L}\"] = y.shift(L).astype(float)\n",
    "    return exog\n",
    "\n",
    "def build_lag_regressors_from_history(future_idx: pd.DatetimeIndex, history: pd.Series, lags=LAGS) -> pd.DataFrame:\n",
    "    exog = pd.DataFrame(index=future_idx)\n",
    "    for L in lags:\n",
    "        exog[f\"y_lag_{L}\"] = [float(history.get(t - pd.Timedelta(hours=L), np.nan)) for t in future_idx]\n",
    "    return exog\n",
    "\n",
    "ART_SARIMAX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ART_PROPHET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sarimax_models = {}\n",
    "prophet_models = {}\n",
    "sarimax_paths = {}\n",
    "prophet_paths = {}\n",
    "sarimax_scales = {}   # store scaling per cluster (important)\n",
    "sarimax_used_spec = {}\n",
    "\n",
    "for c in CLUSTERS:\n",
    "    print(f\"[INFO] Training cluster {c} SARIMAX (departures) + Prophet (arrivals)...\")\n",
    "\n",
    "    # ---------------- SARIMAX (departures) ----------------\n",
    "    y_dep = series[c][\"departures\"].astype(float)\n",
    "\n",
    "    exog_dep = build_lag_regressors_full(y_dep, lags=LAGS)\n",
    "    if USE_US_HOLIDAYS_SARIMAX:\n",
    "        exog_dep[\"is_us_holiday\"] = us_holiday_indicator(y_dep.index).astype(float)\n",
    "\n",
    "    exog_dep_train = exog_dep.dropna()\n",
    "    y_dep_train = y_dep.loc[exog_dep_train.index]\n",
    "\n",
    "    if len(y_dep_train) < 10:\n",
    "        raise ValueError(f\"Not enough data to train SARIMAX for cluster {c}\")\n",
    "\n",
    "    # simple scaling to help convergence + conditioning\n",
    "    scale = float(np.nanmax(y_dep_train.values))\n",
    "    if not np.isfinite(scale) or scale <= 0:\n",
    "        scale = 1.0\n",
    "    sarimax_scales[c] = scale\n",
    "\n",
    "    y_s = y_dep_train / scale\n",
    "    X_s = exog_dep_train / scale\n",
    "    if USE_US_HOLIDAYS_SARIMAX:\n",
    "        X_s[\"is_us_holiday\"] = exog_dep_train[\"is_us_holiday\"]  # don't scale binary\n",
    "\n",
    "    primary_spec = dict(order=(1, 1, 2), seasonal_order=(1, 0, 1, 24))\n",
    "    fallback_spec = dict(order=(0, 1, 1), seasonal_order=(1, 0, 1, 24))  # cheaper + often stable\n",
    "\n",
    "    def _fit_one(spec):\n",
    "        model = SARIMAX(\n",
    "            y_s, exog=X_s,\n",
    "            enforce_stationarity=True,     # helps convergence\n",
    "            enforce_invertibility=True,    # helps convergence\n",
    "            **spec\n",
    "        )\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "            res = model.fit(disp=False, method=\"lbfgs\", maxiter=80)\n",
    "        return res\n",
    "\n",
    "    try:\n",
    "        sar_res = _fit_one(primary_spec)\n",
    "        converged = bool(sar_res.mle_retvals.get(\"converged\", False))\n",
    "        if not converged:\n",
    "            # one cheap fallback only\n",
    "            sar_res2 = _fit_one(fallback_spec)\n",
    "            if bool(sar_res2.mle_retvals.get(\"converged\", False)):\n",
    "                sar_res = sar_res2\n",
    "                sarimax_used_spec[c] = fallback_spec\n",
    "            else:\n",
    "                sarimax_used_spec[c] = primary_spec\n",
    "        else:\n",
    "            sarimax_used_spec[c] = primary_spec\n",
    "    except Exception:\n",
    "        # if primary fails hard, try fallback once\n",
    "        sar_res = _fit_one(fallback_spec)\n",
    "        sarimax_used_spec[c] = fallback_spec\n",
    "\n",
    "    sarimax_models[c] = sar_res\n",
    "\n",
    "    sar_path = ART_SARIMAX_DIR / f\"sarimax_departures_model_{c}.pkl\"\n",
    "    with open(sar_path, \"wb\") as f:\n",
    "        # store both model + scale (so forecasts can be unscaled later)\n",
    "        pickle.dump({\"res\": sar_res, \"scale\": sarimax_scales[c], \"spec\": sarimax_used_spec[c]}, f)\n",
    "    sarimax_paths[c] = sar_path\n",
    "\n",
    "    conv_flag = bool(sar_res.mle_retvals.get(\"converged\", False))\n",
    "    print(f\"[INFO] cluster {c}: SARIMAX converged={conv_flag}, spec={sarimax_used_spec[c]}, scale={sarimax_scales[c]:.3g}\")\n",
    "\n",
    "    # ---------------- Prophet (arrivals) ----------------\n",
    "    y_arr = series[c][\"arrivals\"].astype(float)\n",
    "    df_prop = pd.DataFrame({\"ds\": y_arr.index, \"y\": y_arr.values})\n",
    "    df_prop = prophet_conditions(df_prop)\n",
    "\n",
    "    if USE_LAGS_PROPHET:\n",
    "        for L in LAGS:\n",
    "            df_prop[f\"y_lag_{L}\"] = y_arr.shift(L).values\n",
    "\n",
    "    df_prop_train = df_prop.dropna()\n",
    "    if len(df_prop_train) < 10:\n",
    "        raise ValueError(f\"Not enough data to train Prophet for cluster {c}\")\n",
    "\n",
    "    m = Prophet(\n",
    "        changepoint_prior_scale=0.2,\n",
    "        seasonality_prior_scale=10,\n",
    "        seasonality_mode=\"additive\",\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False,\n",
    "    )\n",
    "\n",
    "    # FIX: conditional seasonalities instead of collinear weekday/weekend regressors\n",
    "    m.add_seasonality(name=\"daily_weekday\", period=1, fourier_order=15, condition_name=\"is_weekday\")\n",
    "    m.add_seasonality(name=\"daily_weekend\", period=1, fourier_order=15, condition_name=\"is_weekend\")\n",
    "    m.add_seasonality(name=\"weekly\", period=7, fourier_order=9)\n",
    "\n",
    "    if USE_LAGS_PROPHET:\n",
    "        for L in LAGS:\n",
    "            m.add_regressor(f\"y_lag_{L}\", prior_scale=10)\n",
    "\n",
    "    m.fit(df_prop_train)\n",
    "    prophet_models[c] = m\n",
    "\n",
    "    prop_path = ART_PROPHET_DIR / f\"prophet_model_{c}_prophet_additive_cps0.2_sps10_condseas.json\"\n",
    "    prop_path.write_text(model_to_json(m))\n",
    "    prophet_paths[c] = prop_path\n",
    "\n",
    "print(\"[OK] Trained and saved SARIMAX + Prophet models for all clusters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "945b93d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def us_holiday_indicator(idx: pd.DatetimeIndex) -> pd.Series:\n",
    "    cal = USFederalHolidayCalendar()\n",
    "    hols = cal.holidays(start=idx.min().floor(\"D\"), end=idx.max().ceil(\"D\"))\n",
    "    return idx.floor(\"D\").isin(hols).astype(int)\n",
    "\n",
    "def prophet_conditions(df_future: pd.DataFrame) -> pd.DataFrame:\n",
    "    dow = df_future[\"ds\"].dt.dayofweek\n",
    "    df_future[\"is_weekend\"] = (dow >= 5)\n",
    "    df_future[\"is_weekday\"] = (dow < 5)\n",
    "    return df_future\n",
    "\n",
    "def build_lag_regressors_from_history(future_idx: pd.DatetimeIndex, history: pd.Series, lags=LAGS) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each future timestamp t, create y_lag_L = history[t-L].\n",
    "    This is leak-free if history only contains values <= cutoff-1h and t is in next-day horizon.\n",
    "    \"\"\"\n",
    "    exog = pd.DataFrame(index=future_idx)\n",
    "    for L in lags:\n",
    "        exog[f\"y_lag_{L}\"] = [float(history.get(t - pd.Timedelta(hours=L), np.nan)) for t in future_idx]\n",
    "    return exog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0970ab9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] #val days: 30 first: 2018-11-01 00:00:00 last: 2018-11-30 00:00:00\n",
      "[INFO] #test days: 31 first: 2018-12-01 00:00:00 last: 2018-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "def list_cutoffs_for_months(year=2018, months=[10,11,12]) -> list[pd.Timestamp]:\n",
    "    cutoffs = []\n",
    "    for m in months:\n",
    "        start = pd.Timestamp(year=year, month=m, day=1, hour=0)\n",
    "        if m == 12:\n",
    "            end = pd.Timestamp(year=2019, month=1, day=1, hour=0)\n",
    "        else:\n",
    "            end = pd.Timestamp(year=year, month=m+1, day=1, hour=0)\n",
    "        # these are cutoffs at midnight for each day\n",
    "        cutoffs.extend(list(pd.date_range(start, end - pd.Timedelta(days=1), freq=\"D\")))\n",
    "    return cutoffs\n",
    "\n",
    "val_cutoffs = list_cutoffs_for_months(2018, [VAL_MONTH])\n",
    "test_cutoffs = list_cutoffs_for_months(2018, TEST_MONTHS)\n",
    "\n",
    "print(\"[INFO] #val days:\", len(val_cutoffs), \"first:\", val_cutoffs[0], \"last:\", val_cutoffs[-1])\n",
    "print(\"[INFO] #test days:\", len(test_cutoffs), \"first:\", test_cutoffs[0], \"last:\", test_cutoffs[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f96d5bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarimax_daily_forecast(res_or_bundle, history_y: pd.Series, cutoff: pd.Timestamp) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Forecast next 24 hours starting at cutoff using SARIMAX results.\n",
    "    Supports either a plain SARIMAXResults OR the dict bundle saved in Cell 4.\n",
    "    \"\"\"\n",
    "    horizon = pd.date_range(cutoff, cutoff + pd.Timedelta(hours=23), freq=\"h\")\n",
    "\n",
    "    # handle bundled {res, scale}\n",
    "    if isinstance(res_or_bundle, dict):\n",
    "        res = res_or_bundle[\"res\"]\n",
    "        scale = float(res_or_bundle.get(\"scale\", 1.0))\n",
    "    else:\n",
    "        res = res_or_bundle\n",
    "        scale = float(sarimax_scales.get(getattr(res, \"_cluster_id\", None), 1.0)) if \"sarimax_scales\" in globals() else 1.0\n",
    "\n",
    "    exog = build_lag_regressors_from_history(horizon, history_y, lags=LAGS)\n",
    "    # match scaling used in training\n",
    "    exog = exog / scale\n",
    "\n",
    "    if USE_US_HOLIDAYS_SARIMAX:\n",
    "        exog[\"is_us_holiday\"] = us_holiday_indicator(horizon)\n",
    "\n",
    "    fc = res.get_forecast(steps=len(horizon), exog=exog)\n",
    "    pred = fc.predicted_mean * scale\n",
    "    pred.index = horizon\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3984ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophet_daily_forecast(model, history_y: pd.Series, cutoff: pd.Timestamp) -> pd.Series:\n",
    "    horizon = pd.date_range(cutoff, cutoff + pd.Timedelta(hours=23), freq=\"h\")\n",
    "    future = pd.DataFrame({\"ds\": horizon})\n",
    "    future = prophet_conditions(future)\n",
    "\n",
    "    if USE_LAGS_PROPHET:\n",
    "        exog = build_lag_regressors_from_history(horizon, history_y, lags=LAGS)\n",
    "        for col in exog.columns:\n",
    "            future[col] = exog[col].values\n",
    "\n",
    "        # If any lag is missing, cannot forecast that day safely\n",
    "        if future[[f\"y_lag_{L}\" for L in LAGS]].isna().any().any():\n",
    "            # return NaNs to be handled upstream\n",
    "            return pd.Series(index=horizon, dtype=float)\n",
    "\n",
    "    fcst = model.predict(future)\n",
    "    yhat = pd.Series(fcst[\"yhat\"].values, index=horizon)\n",
    "    return yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9c00b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] out shape: (2928, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>split</th>\n",
       "      <th>y_true_pickups</th>\n",
       "      <th>y_pred_sarimax_pickups</th>\n",
       "      <th>y_true_dropoffs</th>\n",
       "      <th>y_pred_prophet_dropoffs</th>\n",
       "      <th>se_pickups_sarimax</th>\n",
       "      <th>se_dropoffs_prophet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.393151</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.400663</td>\n",
       "      <td>21.223059</td>\n",
       "      <td>31.352575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.931564</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.721192</td>\n",
       "      <td>9.415297</td>\n",
       "      <td>68.538664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.964974</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.774456</td>\n",
       "      <td>4.141330</td>\n",
       "      <td>38.757403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-11-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.469890</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.944318</td>\n",
       "      <td>0.281017</td>\n",
       "      <td>16.448559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-11-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.954152</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.840037</td>\n",
       "      <td>0.002102</td>\n",
       "      <td>1.345513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  hour  cluster_id split  y_true_pickups  y_pred_sarimax_pickups  \\\n",
       "0 2018-11-01     0           0   val            10.0                5.393151   \n",
       "1 2018-11-01     1           0   val             6.0                2.931564   \n",
       "2 2018-11-01     2           0   val             3.0                0.964974   \n",
       "3 2018-11-01     3           0   val             1.0                0.469890   \n",
       "4 2018-11-01     4           0   val             1.0                0.954152   \n",
       "\n",
       "   y_true_dropoffs  y_pred_prophet_dropoffs  se_pickups_sarimax  \\\n",
       "0             20.0                14.400663           21.223059   \n",
       "1             16.0                 7.721192            9.415297   \n",
       "2              9.0                 2.774456            4.141330   \n",
       "3              6.0                 1.944318            0.281017   \n",
       "4              2.0                 0.840037            0.002102   \n",
       "\n",
       "   se_dropoffs_prophet  \n",
       "0            31.352575  \n",
       "1            68.538664  \n",
       "2            38.757403  \n",
       "3            16.448559  \n",
       "4             1.345513  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "def add_one_split(cluster_id: int, split_name: str, cutoffs: list[pd.Timestamp]):\n",
    "    y_dep = series[cluster_id][\"departures\"]\n",
    "    y_arr = series[cluster_id][\"arrivals\"]\n",
    "\n",
    "    sar_res = sarimax_models[cluster_id]\n",
    "    pr_model = prophet_models[cluster_id]\n",
    "\n",
    "    for cutoff in cutoffs:\n",
    "        # history available up to cutoff-1h\n",
    "        hist_end = cutoff - pd.Timedelta(hours=1)\n",
    "        y_dep_hist = y_dep.loc[:hist_end]\n",
    "        y_arr_hist = y_arr.loc[:hist_end]\n",
    "\n",
    "        # horizon = next 24h of THIS day (starting at cutoff)\n",
    "        horizon = pd.date_range(cutoff, cutoff + pd.Timedelta(hours=23), freq=\"h\")\n",
    "\n",
    "        # ground truth (allowed for evaluation)\n",
    "        y_true_pickups = y_dep.loc[horizon].values\n",
    "        y_true_dropoffs = y_arr.loc[horizon].values\n",
    "\n",
    "        # predictions (leak-free)\n",
    "        y_pred_pick = sarimax_daily_forecast(sar_res, y_dep_hist, cutoff).values\n",
    "        y_pred_drop = prophet_daily_forecast(pr_model, y_arr_hist, cutoff).values\n",
    "\n",
    "        for ts, hr, yt_p, yp_p, yt_d, yp_d in zip(horizon, horizon.hour, y_true_pickups, y_pred_pick, y_true_dropoffs, y_pred_drop):\n",
    "            rows.append({\n",
    "                \"date\": ts.normalize(),\n",
    "                \"hour\": int(hr),\n",
    "                \"cluster_id\": int(cluster_id),\n",
    "                \"split\": split_name,\n",
    "                \"y_true_pickups\": float(yt_p),\n",
    "                \"y_pred_sarimax_pickups\": float(yp_p) if np.isfinite(yp_p) else np.nan,\n",
    "                \"y_true_dropoffs\": float(yt_d),\n",
    "                \"y_pred_prophet_dropoffs\": float(yp_d) if np.isfinite(yp_d) else np.nan,\n",
    "            })\n",
    "\n",
    "for c in CLUSTERS:\n",
    "    add_one_split(c, \"val\", val_cutoffs)\n",
    "    add_one_split(c, \"test\", test_cutoffs)\n",
    "\n",
    "out = pd.DataFrame(rows)\n",
    "\n",
    "# clip negatives (demand can't be negative)\n",
    "out[\"y_pred_sarimax_pickups\"] = out[\"y_pred_sarimax_pickups\"].clip(lower=0)\n",
    "out[\"y_pred_prophet_dropoffs\"] = out[\"y_pred_prophet_dropoffs\"].clip(lower=0)\n",
    "\n",
    "# squared errors (for RMSE)\n",
    "out[\"se_pickups_sarimax\"] = (out[\"y_true_pickups\"] - out[\"y_pred_sarimax_pickups\"])**2\n",
    "out[\"se_dropoffs_prophet\"] = (out[\"y_true_dropoffs\"] - out[\"y_pred_prophet_dropoffs\"])**2\n",
    "\n",
    "print(\"[INFO] out shape:\", out.shape)\n",
    "out.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b51b7b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved cluster 0:\n",
      "   preds_ts_daily/ts_daily_cluster_0_preds.csv\n",
      "   preds_ts_daily/ts_daily_cluster_0_preds.parquet\n",
      "[OK] Saved cluster 8:\n",
      "   preds_ts_daily/ts_daily_cluster_8_preds.csv\n",
      "   preds_ts_daily/ts_daily_cluster_8_preds.parquet\n"
     ]
    }
   ],
   "source": [
    "for c in CLUSTERS:\n",
    "    dfc = out[out[\"cluster_id\"] == c].copy().sort_values([\"date\",\"hour\"]).reset_index(drop=True)\n",
    "\n",
    "    cols = [\n",
    "        \"date\",\"hour\",\"cluster_id\",\"split\",\n",
    "        \"y_true_pickups\",\"y_pred_sarimax_pickups\",\"se_pickups_sarimax\",\n",
    "        \"y_true_dropoffs\",\"y_pred_prophet_dropoffs\",\"se_dropoffs_prophet\",\n",
    "    ]\n",
    "    dfc = dfc[cols]\n",
    "\n",
    "    csv_path = PRED_OUT_DIR / f\"ts_daily_cluster_{c}_preds.csv\"\n",
    "    pq_path  = PRED_OUT_DIR / f\"ts_daily_cluster_{c}_preds.parquet\"\n",
    "\n",
    "    dfc.to_csv(csv_path, index=False)\n",
    "    dfc.to_parquet(pq_path, index=False)\n",
    "\n",
    "    print(f\"[OK] Saved cluster {c}:\")\n",
    "    print(\"  \", csv_path)\n",
    "    print(\"  \", pq_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78fb8424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL: {'rows': 1440, 'rmse_pickups_sarimax': 92.52174598656532, 'rmse_dropoffs_prophet': 85.46599823206896}\n",
      "TEST: {'rows': 1488, 'rmse_pickups_sarimax': 65.82222622225883, 'rmse_dropoffs_prophet': 66.17711050189448}\n"
     ]
    }
   ],
   "source": [
    "def summarize(split):\n",
    "    d = out[out[\"split\"] == split]\n",
    "    # RMSE: sqrt(mean(squared_error))\n",
    "    return {\n",
    "        \"rows\": len(d),\n",
    "        \"rmse_pickups_sarimax\": float(np.sqrt(d[\"se_pickups_sarimax\"].mean())) if len(d) > 0 else float('nan'),\n",
    "        \"rmse_dropoffs_prophet\": float(np.sqrt(d[\"se_dropoffs_prophet\"].mean())) if len(d) > 0 else float('nan'),\n",
    "    }\n",
    "\n",
    "print(\"VAL:\", summarize(\"val\"))\n",
    "print(\"TEST:\", summarize(\"test\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7ab7bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] wrote: artifacts_ts_daily/ts_sarimax_prophet_general_weight.json\n",
      "{\n",
      "  \"model\": \"TS_SARIMAX_Departures__Prophet_Arrivals\",\n",
      "  \"clusters\": [\n",
      "    0,\n",
      "    8\n",
      "  ],\n",
      "  \"general_rmse_val_nov_pickups_sarimax\": 92.52174598656532,\n",
      "  \"general_rmse_val_nov_dropoffs_prophet\": 85.46599823206896,\n",
      "  \"general_rmse_val_nov_combined\": 88.99387210931714,\n",
      "  \"general_weight_raw\": 0.011236728735214445,\n",
      "  \"val_start\": \"2018-11-01\",\n",
      "  \"val_end\": \"2018-12-01\",\n",
      "  \"target_departures_model\": \"SARIMAX\",\n",
      "  \"target_arrivals_model\": \"Prophet\",\n",
      "  \"lags_hours\": [\n",
      "    24,\n",
      "    168\n",
      "  ],\n",
      "  \"use_us_holidays_sarimax\": true,\n",
      "  \"prophet_conditional_daily_seasonality\": true,\n",
      "  \"prophet_weekly_seasonality\": true,\n",
      "  \"sarimax_dir\": \"artifacts_sarimax_daily\",\n",
      "  \"prophet_dir\": \"artifacts_prophet_daily\",\n",
      "  \"dep_model_paths_by_cluster\": {\n",
      "    \"0\": \"artifacts_sarimax_daily/sarimax_departures_model_0.pkl\",\n",
      "   \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Cell X: write a \"general weights\" json for this SARIMAX+Prophet (per-cluster) setup\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "TS_WEIGHT_DIR = Path(\"artifacts_ts_daily\")\n",
    "TS_WEIGHT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _rmse(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    m = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if m.sum() == 0:\n",
    "        return float(\"nan\")\n",
    "    return float(np.sqrt(np.mean((y_true[m] - y_pred[m]) ** 2)))\n",
    "\n",
    "# Use 'out' produced in Cell 9 (contains val/test rows & squared errors)\n",
    "val_df = out[out[\"split\"] == \"val\"].copy()\n",
    "\n",
    "rmse_pickups = _rmse(val_df[\"y_true_pickups\"], val_df[\"y_pred_sarimax_pickups\"])\n",
    "rmse_dropoffs = _rmse(val_df[\"y_true_dropoffs\"], val_df[\"y_pred_prophet_dropoffs\"])\n",
    "\n",
    "# Optional: a single combined score (useful for \"best model\" selection if you want one number)\n",
    "rmse_combined = float(np.nanmean([rmse_pickups, rmse_dropoffs]))\n",
    "\n",
    "val_start = pd.Timestamp(year=2018, month=VAL_MONTH, day=1).strftime(\"%Y-%m-%d\")\n",
    "val_end = (pd.Timestamp(year=2018, month=VAL_MONTH, day=1) + pd.offsets.MonthBegin(1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"TS_SARIMAX_Departures__Prophet_Arrivals\",\n",
    "    \"clusters\": list(map(int, CLUSTERS)),\n",
    "    \"general_rmse_val_nov_pickups_sarimax\": rmse_pickups,\n",
    "    \"general_rmse_val_nov_dropoffs_prophet\": rmse_dropoffs,\n",
    "    \"general_rmse_val_nov_combined\": rmse_combined,\n",
    "\n",
    "    # A \"weight-like\" scalar mirroring your other jsons (bigger rmse -> smaller weight)\n",
    "    # (not used for training; just for comparison / selection logic)\n",
    "    \"general_weight_raw\": float(1.0 / (rmse_combined + 1e-9)) if np.isfinite(rmse_combined) else float(\"nan\"),\n",
    "\n",
    "    \"val_start\": val_start,\n",
    "    \"val_end\": val_end,\n",
    "\n",
    "    # What this TS model uses\n",
    "    \"target_departures_model\": \"SARIMAX\",\n",
    "    \"target_arrivals_model\": \"Prophet\",\n",
    "    \"lags_hours\": list(map(int, LAGS)),\n",
    "    \"use_us_holidays_sarimax\": bool(USE_US_HOLIDAYS_SARIMAX),\n",
    "    \"prophet_conditional_daily_seasonality\": True,\n",
    "    \"prophet_weekly_seasonality\": True,\n",
    "\n",
    "    # Where the per-cluster artifacts live\n",
    "    \"sarimax_dir\": str(ART_SARIMAX_DIR.as_posix()),\n",
    "    \"prophet_dir\": str(ART_PROPHET_DIR.as_posix()),\n",
    "\n",
    "    # Exact filenames by cluster\n",
    "    \"dep_model_paths_by_cluster\": {str(int(c)): str((ART_SARIMAX_DIR / f\"sarimax_departures_model_{c}.pkl\").as_posix()) for c in CLUSTERS},\n",
    "    \"arr_model_paths_by_cluster\": {str(int(c)): str((ART_PROPHET_DIR / f\"prophet_model_{c}_prophet_additive_cps0.2_sps10_condseas.json\").as_posix()) for c in CLUSTERS},\n",
    "}\n",
    "\n",
    "out_path = TS_WEIGHT_DIR / \"ts_sarimax_prophet_general_weight.json\"\n",
    "out_path.write_text(json.dumps(payload, indent=2))\n",
    "print(\"[OK] wrote:\", out_path)\n",
    "print(json.dumps(payload, indent=2)[:800] + \"\\n...\")  # preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "618afb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Exported TS parquet for cluster 0\n",
      "[OK] Exported TS parquet for cluster 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>val_rmse_pickups</th>\n",
       "      <th>val_rmse_dropoffs</th>\n",
       "      <th>val_rmse_mean</th>\n",
       "      <th>parquet_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12.513269</td>\n",
       "      <td>10.213944</td>\n",
       "      <td>8.029918</td>\n",
       "      <td>preds_ts_daily_parquet_like_other_models/ts_cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>130.245787</td>\n",
       "      <td>120.434833</td>\n",
       "      <td>79.691670</td>\n",
       "      <td>preds_ts_daily_parquet_like_other_models/ts_cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster_id  val_rmse_pickups  val_rmse_dropoffs  val_rmse_mean  \\\n",
       "0           0         12.513269          10.213944       8.029918   \n",
       "1           8        130.245787         120.434833      79.691670   \n",
       "\n",
       "                                        parquet_path  \n",
       "0  preds_ts_daily_parquet_like_other_models/ts_cl...  \n",
       "1  preds_ts_daily_parquet_like_other_models/ts_cl...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell X: Export per-cluster .parquet with the same \"pickups/dropoffs\" schema style\n",
    "# Uses the 'out' dataframe produced by Cell 9 (SARIMAX pickups, Prophet dropoffs)\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "TS_PRED_DIR = Path(\"preds_ts_daily_parquet_like_other_models\")\n",
    "TS_PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def export_cluster_ts(out_df: pd.DataFrame, cluster_id: int):\n",
    "    d = out_df[out_df[\"cluster_id\"] == cluster_id].copy()\n",
    "\n",
    "    val = d[d[\"split\"] == \"val\"].copy()\n",
    "    test = d[d[\"split\"] == \"test\"].copy()\n",
    "\n",
    "    if len(val) == 0:\n",
    "        raise ValueError(f\"Cluster {cluster_id}: empty validation slice.\")\n",
    "    if len(test) == 0:\n",
    "        print(f\"[WARN] Cluster {cluster_id}: empty test slice.\")\n",
    "\n",
    "    def finalize(block: pd.DataFrame, split_name: str) -> pd.DataFrame:\n",
    "        outp = pd.DataFrame({\n",
    "            \"date\": block[\"date\"].values,\n",
    "            \"hour\": block[\"hour\"].values,\n",
    "            \"cluster_id\": block[\"cluster_id\"].values,\n",
    "            \"split\": split_name,\n",
    "\n",
    "            # match your other-model schema naming\n",
    "            \"y_true_pickups\": block[\"y_true_pickups\"].values,\n",
    "            \"y_pred_ts_pickups\": block[\"y_pred_sarimax_pickups\"].values,\n",
    "\n",
    "            \"y_true_dropoffs\": block[\"y_true_dropoffs\"].values,\n",
    "            \"y_pred_ts_dropoffs\": block[\"y_pred_prophet_dropoffs\"].values,\n",
    "        })\n",
    "\n",
    "        # squared errors\n",
    "        outp[\"se_pickups_ts\"] = (outp[\"y_true_pickups\"] - outp[\"y_pred_ts_pickups\"]) ** 2\n",
    "        outp[\"se_dropoffs_ts\"] = (outp[\"y_true_dropoffs\"] - outp[\"y_pred_ts_dropoffs\"]) ** 2\n",
    "\n",
    "        # mean RMSE per row (same idea as your rf/mlp schema)\n",
    "        out_mse = 0.5 * (outp[\"se_pickups_ts\"] + outp[\"se_dropoffs_ts\"])\n",
    "        outp[\"rmse_mean_ts\"] = np.sqrt(out_mse)\n",
    "\n",
    "        return outp\n",
    "\n",
    "    df_val_out = finalize(val, \"val\")\n",
    "    df_test_out = finalize(test, \"test\") if len(test) > 0 else pd.DataFrame(columns=df_val_out.columns)\n",
    "\n",
    "    df_out = pd.concat([df_val_out, df_test_out], ignore_index=True)\n",
    "\n",
    "    parquet_path = TS_PRED_DIR / f\"ts_cluster_{cluster_id}_preds.parquet\"\n",
    "    df_out.to_parquet(parquet_path, index=False)\n",
    "\n",
    "    # summaries like your other pipeline\n",
    "    rmse_val_pickups = float(np.sqrt(df_val_out[\"se_pickups_ts\"].mean()))\n",
    "    rmse_val_dropoffs = float(np.sqrt(df_val_out[\"se_dropoffs_ts\"].mean()))\n",
    "    rmse_val_mean = float(df_val_out[\"rmse_mean_ts\"].mean())\n",
    "\n",
    "    return {\n",
    "        \"cluster_id\": int(cluster_id),\n",
    "        \"val_rmse_pickups\": rmse_val_pickups,\n",
    "        \"val_rmse_dropoffs\": rmse_val_dropoffs,\n",
    "        \"val_rmse_mean\": rmse_val_mean,\n",
    "        \"parquet_path\": str(parquet_path),\n",
    "    }\n",
    "\n",
    "summaries = []\n",
    "for cid in CLUSTERS:\n",
    "    try:\n",
    "        summaries.append(export_cluster_ts(out, cid))\n",
    "        print(f\"[OK] Exported TS parquet for cluster {cid}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"[WARN] {e}\")\n",
    "\n",
    "df_summary = pd.DataFrame(summaries).sort_values(\"cluster_id\").reset_index(drop=True)\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167c3ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

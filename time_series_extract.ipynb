{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72855321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Outputs -> /mnt/data/Desktop/Masters/Business_Analytics/Intro_to_BA/preds_ts_daily\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from prophet.serialize import model_from_json\n",
    "\n",
    "# ================== INPUTS ==================\n",
    "HOURLY_PATH = Path(\"cluster_hourly.parquet\")\n",
    "\n",
    "SARIMAX_DIR = Path(\"sarimax/sarimax_best\")\n",
    "PROPHET_DIR = Path(\"prophet_folder/prophet_best\")\n",
    "\n",
    "CLUSTERS = [0, 1]\n",
    "\n",
    "# We evaluate day-ahead forecasts for these months\n",
    "VAL_MONTH = 10           # October\n",
    "TEST_MONTHS = [11, 12]   # Nov-Dec\n",
    "\n",
    "HARD_END = pd.Timestamp(\"2018-12-31 23:00:00\")\n",
    "\n",
    "# Regressors used in training scripts\n",
    "LAGS = [24, 168]  # hours\n",
    "USE_US_HOLIDAYS_SARIMAX = True\n",
    "\n",
    "# Prophet used weekend/weekday conditional seasonality + lag regressors\n",
    "USE_LAGS_PROPHET = True\n",
    "\n",
    "# ================== OUTPUTS ==================\n",
    "PRED_OUT_DIR = Path(\"preds_ts_daily\")\n",
    "ART_SARIMAX_DIR = Path(\"artifacts_sarimax_daily\")\n",
    "ART_PROPHET_DIR = Path(\"artifacts_prophet_daily\")\n",
    "PRED_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ART_SARIMAX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ART_PROPHET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[INFO] Outputs ->\", PRED_OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0feee6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded: (183960, 10)\n",
      "[INFO] Columns: ['cluster', 'datetime_hour', 'departures', 'arrivals', 'was_missing_departures', 'was_missing_arrivals', 'weekday', 'hour', 'month', 'year']\n",
      "[INFO] After filtering: (17520, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>datetime_hour</th>\n",
       "      <th>departures</th>\n",
       "      <th>arrivals</th>\n",
       "      <th>was_missing_departures</th>\n",
       "      <th>was_missing_arrivals</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster       datetime_hour  departures  arrivals  was_missing_departures  \\\n",
       "0        0 2018-01-01 00:00:00         3.0       1.0                   False   \n",
       "1        0 2018-01-01 01:00:00         2.0       4.0                   False   \n",
       "2        0 2018-01-01 02:00:00         3.0       2.0                   False   \n",
       "3        0 2018-01-01 03:00:00         4.0       7.0                   False   \n",
       "4        0 2018-01-01 04:00:00         0.0       1.0                   False   \n",
       "\n",
       "   was_missing_arrivals  weekday  hour  month  year  \n",
       "0                 False        0     0      1  2018  \n",
       "1                 False        0     1      1  2018  \n",
       "2                 False        0     2      1  2018  \n",
       "3                 False        0     3      1  2018  \n",
       "4                 False        0     4      1  2018  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(HOURLY_PATH).copy()\n",
    "print(\"[INFO] Loaded:\", df.shape)\n",
    "print(\"[INFO] Columns:\", list(df.columns))\n",
    "\n",
    "# cluster\n",
    "if \"cluster\" not in df.columns:\n",
    "    if \"cluster_id\" in df.columns:\n",
    "        df = df.rename(columns={\"cluster_id\":\"cluster\"})\n",
    "    elif \"gmm20_cluster\" in df.columns:\n",
    "        df = df.rename(columns={\"gmm20_cluster\":\"cluster\"})\n",
    "    else:\n",
    "        raise ValueError(\"Need cluster column: 'cluster' or 'cluster_id' or 'gmm20_cluster'.\")\n",
    "\n",
    "# datetime_hour\n",
    "if \"datetime_hour\" not in df.columns:\n",
    "    if \"timestamp\" in df.columns:\n",
    "        df = df.rename(columns={\"timestamp\":\"datetime_hour\"})\n",
    "    elif \"datetime\" in df.columns:\n",
    "        df = df.rename(columns={\"datetime\":\"datetime_hour\"})\n",
    "    elif {\"date\",\"hour\"}.issubset(df.columns):\n",
    "        df[\"datetime_hour\"] = pd.to_datetime(df[\"date\"]).dt.normalize() + pd.to_timedelta(df[\"hour\"], unit=\"h\")\n",
    "    else:\n",
    "        raise ValueError(\"Need time column: 'datetime_hour' or 'timestamp'/'datetime' or ('date','hour').\")\n",
    "\n",
    "df[\"datetime_hour\"] = pd.to_datetime(df[\"datetime_hour\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"datetime_hour\"]).copy()\n",
    "\n",
    "# departures/arrivals\n",
    "if \"departures\" not in df.columns:\n",
    "    if \"pickups\" in df.columns:\n",
    "        df = df.rename(columns={\"pickups\":\"departures\"})\n",
    "    else:\n",
    "        raise ValueError(\"Need departures column: 'departures' or 'pickups'.\")\n",
    "\n",
    "if \"arrivals\" not in df.columns:\n",
    "    if \"dropoffs\" in df.columns:\n",
    "        df = df.rename(columns={\"dropoffs\":\"arrivals\"})\n",
    "    else:\n",
    "        raise ValueError(\"Need arrivals column: 'arrivals' or 'dropoffs'.\")\n",
    "\n",
    "# filter\n",
    "df = df[df[\"cluster\"].isin(CLUSTERS)].copy()\n",
    "df = df[df[\"datetime_hour\"] <= HARD_END].copy()\n",
    "df = df.sort_values([\"cluster\",\"datetime_hour\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"[INFO] After filtering:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d7c9f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] cluster 0: departures hours=8760, arrivals hours=8760\n",
      "[INFO] cluster 1: departures hours=8760, arrivals hours=8760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_809108/2085566942.py:7: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  s = out.set_index(\"ds\")[col_y].asfreq(\"H\")\n",
      "/tmp/ipykernel_809108/2085566942.py:7: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  s = out.set_index(\"ds\")[col_y].asfreq(\"H\")\n",
      "/tmp/ipykernel_809108/2085566942.py:7: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  s = out.set_index(\"ds\")[col_y].asfreq(\"H\")\n",
      "/tmp/ipykernel_809108/2085566942.py:7: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  s = out.set_index(\"ds\")[col_y].asfreq(\"H\")\n"
     ]
    }
   ],
   "source": [
    "def make_dense_series(df_c: pd.DataFrame, col_y: str) -> pd.Series:\n",
    "    tmp = df_c[[\"datetime_hour\", col_y]].copy()\n",
    "    tmp = tmp.groupby(\"datetime_hour\", as_index=False)[col_y].sum().sort_values(\"datetime_hour\")\n",
    "    idx = pd.date_range(tmp[\"datetime_hour\"].min(), tmp[\"datetime_hour\"].max(), freq=\"h\")\n",
    "    out = pd.DataFrame({\"ds\": idx}).merge(tmp.rename(columns={\"datetime_hour\":\"ds\"}), on=\"ds\", how=\"left\")\n",
    "    out[col_y] = out[col_y].fillna(0.0).astype(float)\n",
    "    s = out.set_index(\"ds\")[col_y].asfreq(\"H\")\n",
    "    if s.isna().any():\n",
    "        raise ValueError(\"Dense series still has NaNs â€” unexpected.\")\n",
    "    return s\n",
    "\n",
    "series = {}\n",
    "for c in CLUSTERS:\n",
    "    df_c = df[df[\"cluster\"] == c].copy()\n",
    "    series[c] = {\n",
    "        \"departures\": make_dense_series(df_c, \"departures\"),\n",
    "        \"arrivals\": make_dense_series(df_c, \"arrivals\"),\n",
    "    }\n",
    "    print(f\"[INFO] cluster {c}: departures hours={len(series[c]['departures'])}, arrivals hours={len(series[c]['arrivals'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7d7d27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARIMAX paths: {0: 'sarimax_departures_model_0_sarimax_dep_p1d1q2_P1D0Q1_S24.pkl', 1: 'sarimax_departures_model_1_sarimax_dep_p1d1q1_P1D0Q1_S24.pkl'}\n",
      "PROPHET paths: {0: 'prophet_model_0_prophet_additive_cps0.2_sps10_dfo25_wfo9.json', 1: 'prophet_model_1_prophet_additive_cps0.2_sps5_dfo25_wfo10.json'}\n",
      "[OK] Loaded SARIMAX + Prophet models.\n"
     ]
    }
   ],
   "source": [
    "def find_one(pattern: str, base: Path) -> Path:\n",
    "    m = sorted(base.glob(pattern))\n",
    "    if not m:\n",
    "        raise FileNotFoundError(f\"No match for {pattern} in {base}\")\n",
    "    return m[0]\n",
    "\n",
    "sarimax_paths = {c: find_one(f\"sarimax_departures_model_{c}_*.pkl\", SARIMAX_DIR) for c in CLUSTERS}\n",
    "prophet_paths = {c: find_one(f\"prophet_model_{c}_*.json\", PROPHET_DIR) for c in CLUSTERS}\n",
    "\n",
    "print(\"SARIMAX paths:\", {k:v.name for k,v in sarimax_paths.items()})\n",
    "print(\"PROPHET paths:\", {k:v.name for k,v in prophet_paths.items()})\n",
    "\n",
    "sarimax_models = {}\n",
    "for c,p in sarimax_paths.items():\n",
    "    with open(p, \"rb\") as f:\n",
    "        sarimax_models[c] = pickle.load(f)\n",
    "\n",
    "prophet_models = {}\n",
    "for c,p in prophet_paths.items():\n",
    "    prophet_models[c] = model_from_json(p.read_text())\n",
    "\n",
    "print(\"[OK] Loaded SARIMAX + Prophet models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "945b93d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def us_holiday_indicator(idx: pd.DatetimeIndex) -> pd.Series:\n",
    "    cal = USFederalHolidayCalendar()\n",
    "    hols = cal.holidays(start=idx.min().floor(\"D\"), end=idx.max().ceil(\"D\"))\n",
    "    return idx.floor(\"D\").isin(hols).astype(int)\n",
    "\n",
    "def prophet_conditions(df_future: pd.DataFrame) -> pd.DataFrame:\n",
    "    dow = df_future[\"ds\"].dt.dayofweek\n",
    "    df_future[\"is_weekend\"] = (dow >= 5)\n",
    "    df_future[\"is_weekday\"] = (dow < 5)\n",
    "    return df_future\n",
    "\n",
    "def build_lag_regressors_from_history(future_idx: pd.DatetimeIndex, history: pd.Series, lags=LAGS) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each future timestamp t, create y_lag_L = history[t-L].\n",
    "    This is leak-free if history only contains values <= cutoff-1h and t is in next-day horizon.\n",
    "    \"\"\"\n",
    "    exog = pd.DataFrame(index=future_idx)\n",
    "    for L in lags:\n",
    "        exog[f\"y_lag_{L}\"] = [float(history.get(t - pd.Timedelta(hours=L), np.nan)) for t in future_idx]\n",
    "    return exog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0970ab9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] #val days: 31 first: 2018-10-01 00:00:00 last: 2018-10-31 00:00:00\n",
      "[INFO] #test days: 61 first: 2018-11-01 00:00:00 last: 2018-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "def list_cutoffs_for_months(year=2018, months=[10,11,12]) -> list[pd.Timestamp]:\n",
    "    cutoffs = []\n",
    "    for m in months:\n",
    "        start = pd.Timestamp(year=year, month=m, day=1, hour=0)\n",
    "        if m == 12:\n",
    "            end = pd.Timestamp(year=2019, month=1, day=1, hour=0)\n",
    "        else:\n",
    "            end = pd.Timestamp(year=year, month=m+1, day=1, hour=0)\n",
    "        # these are cutoffs at midnight for each day\n",
    "        cutoffs.extend(list(pd.date_range(start, end - pd.Timedelta(days=1), freq=\"D\")))\n",
    "    return cutoffs\n",
    "\n",
    "val_cutoffs = list_cutoffs_for_months(2018, [VAL_MONTH])\n",
    "test_cutoffs = list_cutoffs_for_months(2018, TEST_MONTHS)\n",
    "\n",
    "print(\"[INFO] #val days:\", len(val_cutoffs), \"first:\", val_cutoffs[0], \"last:\", val_cutoffs[-1])\n",
    "print(\"[INFO] #test days:\", len(test_cutoffs), \"first:\", test_cutoffs[0], \"last:\", test_cutoffs[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f96d5bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarimax_daily_forecast(res, history_y: pd.Series, cutoff: pd.Timestamp) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Forecast next 24 hours starting at cutoff using SARIMAX results.\n",
    "    Exog: y_lag_24/y_lag_168 (from history_y) + is_us_holiday.\n",
    "    \"\"\"\n",
    "    horizon = pd.date_range(cutoff, cutoff + pd.Timedelta(hours=23), freq=\"h\")\n",
    "    exog = build_lag_regressors_from_history(horizon, history_y, lags=LAGS)\n",
    "\n",
    "    if USE_US_HOLIDAYS_SARIMAX:\n",
    "        exog[\"is_us_holiday\"] = us_holiday_indicator(horizon)\n",
    "\n",
    "\n",
    "    # SARIMAX expects 2D array-like exog aligned with steps\n",
    "    fc = res.get_forecast(steps=len(horizon), exog=exog)\n",
    "    pred = fc.predicted_mean\n",
    "    pred.index = horizon\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3984ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophet_daily_forecast(model, history_y: pd.Series, cutoff: pd.Timestamp) -> pd.Series:\n",
    "    horizon = pd.date_range(cutoff, cutoff + pd.Timedelta(hours=23), freq=\"h\")\n",
    "    future = pd.DataFrame({\"ds\": horizon})\n",
    "    future = prophet_conditions(future)\n",
    "\n",
    "    if USE_LAGS_PROPHET:\n",
    "        exog = build_lag_regressors_from_history(horizon, history_y, lags=LAGS)\n",
    "        for col in exog.columns:\n",
    "            future[col] = exog[col].values\n",
    "\n",
    "        # If any lag is missing, cannot forecast that day safely\n",
    "        if future[[f\"y_lag_{L}\" for L in LAGS]].isna().any().any():\n",
    "            # return NaNs to be handled upstream\n",
    "            return pd.Series(index=horizon, dtype=float)\n",
    "\n",
    "    fcst = model.predict(future)\n",
    "    yhat = pd.Series(fcst[\"yhat\"].values, index=horizon)\n",
    "    return yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9c00b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] out shape: (4416, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>split</th>\n",
       "      <th>y_true_pickups</th>\n",
       "      <th>y_pred_sarimax_pickups</th>\n",
       "      <th>y_true_dropoffs</th>\n",
       "      <th>y_pred_prophet_dropoffs</th>\n",
       "      <th>ae_pickups_sarimax</th>\n",
       "      <th>ae_dropoffs_prophet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.343159</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.783953</td>\n",
       "      <td>1.343159</td>\n",
       "      <td>3.783953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.754439</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.974062</td>\n",
       "      <td>3.754439</td>\n",
       "      <td>1.025938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.230385</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.888411</td>\n",
       "      <td>0.769615</td>\n",
       "      <td>2.888411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.840597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.387385</td>\n",
       "      <td>0.159403</td>\n",
       "      <td>5.387385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.569315</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.569315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  hour  cluster_id split  y_true_pickups  y_pred_sarimax_pickups  \\\n",
       "0 2018-10-01     0           0   val            15.0               16.343159   \n",
       "1 2018-10-01     1           0   val             6.0                9.754439   \n",
       "2 2018-10-01     2           0   val             3.0                2.230385   \n",
       "3 2018-10-01     3           0   val             1.0                0.840597   \n",
       "4 2018-10-01     4           0   val             2.0                0.000000   \n",
       "\n",
       "   y_true_dropoffs  y_pred_prophet_dropoffs  ae_pickups_sarimax  \\\n",
       "0             15.0                18.783953            1.343159   \n",
       "1             11.0                 9.974062            3.754439   \n",
       "2              3.0                 5.888411            0.769615   \n",
       "3              0.0                 5.387385            0.159403   \n",
       "4              3.0                 4.569315            2.000000   \n",
       "\n",
       "   ae_dropoffs_prophet  \n",
       "0             3.783953  \n",
       "1             1.025938  \n",
       "2             2.888411  \n",
       "3             5.387385  \n",
       "4             1.569315  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "def add_one_split(cluster_id: int, split_name: str, cutoffs: list[pd.Timestamp]):\n",
    "    y_dep = series[cluster_id][\"departures\"]\n",
    "    y_arr = series[cluster_id][\"arrivals\"]\n",
    "\n",
    "    sar_res = sarimax_models[cluster_id]\n",
    "    pr_model = prophet_models[cluster_id]\n",
    "\n",
    "    for cutoff in cutoffs:\n",
    "        # history available up to cutoff-1h\n",
    "        hist_end = cutoff - pd.Timedelta(hours=1)\n",
    "        y_dep_hist = y_dep.loc[:hist_end]\n",
    "        y_arr_hist = y_arr.loc[:hist_end]\n",
    "\n",
    "        # horizon = next 24h of THIS day (starting at cutoff)\n",
    "        horizon = pd.date_range(cutoff, cutoff + pd.Timedelta(hours=23), freq=\"h\")\n",
    "\n",
    "        # ground truth (allowed for evaluation)\n",
    "        y_true_pickups = y_dep.loc[horizon].values\n",
    "        y_true_dropoffs = y_arr.loc[horizon].values\n",
    "\n",
    "        # predictions (leak-free)\n",
    "        y_pred_pick = sarimax_daily_forecast(sar_res, y_dep_hist, cutoff).values\n",
    "        y_pred_drop = prophet_daily_forecast(pr_model, y_arr_hist, cutoff).values\n",
    "\n",
    "        for ts, hr, yt_p, yp_p, yt_d, yp_d in zip(horizon, horizon.hour, y_true_pickups, y_pred_pick, y_true_dropoffs, y_pred_drop):\n",
    "            rows.append({\n",
    "                \"date\": ts.normalize(),\n",
    "                \"hour\": int(hr),\n",
    "                \"cluster_id\": int(cluster_id),\n",
    "                \"split\": split_name,\n",
    "                \"y_true_pickups\": float(yt_p),\n",
    "                \"y_pred_sarimax_pickups\": float(yp_p) if np.isfinite(yp_p) else np.nan,\n",
    "                \"y_true_dropoffs\": float(yt_d),\n",
    "                \"y_pred_prophet_dropoffs\": float(yp_d) if np.isfinite(yp_d) else np.nan,\n",
    "            })\n",
    "\n",
    "for c in CLUSTERS:\n",
    "    add_one_split(c, \"val\", val_cutoffs)\n",
    "    add_one_split(c, \"test\", test_cutoffs)\n",
    "\n",
    "out = pd.DataFrame(rows)\n",
    "\n",
    "# clip negatives (demand can't be negative)\n",
    "out[\"y_pred_sarimax_pickups\"] = out[\"y_pred_sarimax_pickups\"].clip(lower=0)\n",
    "out[\"y_pred_prophet_dropoffs\"] = out[\"y_pred_prophet_dropoffs\"].clip(lower=0)\n",
    "\n",
    "# errors\n",
    "out[\"ae_pickups_sarimax\"] = (out[\"y_true_pickups\"] - out[\"y_pred_sarimax_pickups\"]).abs()\n",
    "out[\"ae_dropoffs_prophet\"] = (out[\"y_true_dropoffs\"] - out[\"y_pred_prophet_dropoffs\"]).abs()\n",
    "\n",
    "print(\"[INFO] out shape:\", out.shape)\n",
    "out.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b51b7b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved cluster 0:\n",
      "   preds_ts_daily/ts_daily_cluster_0_preds.csv\n",
      "   preds_ts_daily/ts_daily_cluster_0_preds.parquet\n",
      "[OK] Saved cluster 1:\n",
      "   preds_ts_daily/ts_daily_cluster_1_preds.csv\n",
      "   preds_ts_daily/ts_daily_cluster_1_preds.parquet\n"
     ]
    }
   ],
   "source": [
    "for c in CLUSTERS:\n",
    "    dfc = out[out[\"cluster_id\"] == c].copy().sort_values([\"date\",\"hour\"]).reset_index(drop=True)\n",
    "\n",
    "    cols = [\n",
    "        \"date\",\"hour\",\"cluster_id\",\"split\",\n",
    "        \"y_true_pickups\",\"y_pred_sarimax_pickups\",\"ae_pickups_sarimax\",\n",
    "        \"y_true_dropoffs\",\"y_pred_prophet_dropoffs\",\"ae_dropoffs_prophet\",\n",
    "    ]\n",
    "    dfc = dfc[cols]\n",
    "\n",
    "    csv_path = PRED_OUT_DIR / f\"ts_daily_cluster_{c}_preds.csv\"\n",
    "    pq_path  = PRED_OUT_DIR / f\"ts_daily_cluster_{c}_preds.parquet\"\n",
    "\n",
    "    dfc.to_csv(csv_path, index=False)\n",
    "    dfc.to_parquet(pq_path, index=False)\n",
    "\n",
    "    print(f\"[OK] Saved cluster {c}:\")\n",
    "    print(\"  \", csv_path)\n",
    "    print(\"  \", pq_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00d627d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Artifacts + metadata written.\n"
     ]
    }
   ],
   "source": [
    "for c in CLUSTERS:\n",
    "    shutil.copy2(sarimax_paths[c], ART_SARIMAX_DIR / sarimax_paths[c].name)\n",
    "    shutil.copy2(prophet_paths[c], ART_PROPHET_DIR / prophet_paths[c].name)\n",
    "\n",
    "sarimax_meta = {\n",
    "    \"model\": \"SARIMAX\",\n",
    "    \"target\": \"pickups\",\n",
    "    \"clusters\": CLUSTERS,\n",
    "    \"lags\": LAGS,\n",
    "    \"use_us_holidays\": USE_US_HOLIDAYS_SARIMAX,\n",
    "    \"evaluation\": \"rolling daily 24h (cutoff at midnight)\",\n",
    "    \"val_month\": VAL_MONTH,\n",
    "    \"test_months\": TEST_MONTHS,\n",
    "    \"files\": {str(c): sarimax_paths[c].name for c in CLUSTERS},\n",
    "}\n",
    "\n",
    "prophet_meta = {\n",
    "    \"model\": \"Prophet\",\n",
    "    \"target\": \"dropoffs\",\n",
    "    \"clusters\": CLUSTERS,\n",
    "    \"lags\": LAGS,\n",
    "    \"evaluation\": \"rolling daily 24h (cutoff at midnight)\",\n",
    "    \"val_month\": VAL_MONTH,\n",
    "    \"test_months\": TEST_MONTHS,\n",
    "    \"files\": {str(c): prophet_paths[c].name for c in CLUSTERS},\n",
    "}\n",
    "\n",
    "(ART_SARIMAX_DIR / \"sarimax_meta.json\").write_text(json.dumps(sarimax_meta, indent=2))\n",
    "(ART_PROPHET_DIR / \"prophet_meta.json\").write_text(json.dumps(prophet_meta, indent=2))\n",
    "\n",
    "print(\"[OK] Artifacts + metadata written.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78fb8424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL: {'rows': 1488, 'mae_pickups_sarimax': 17.78247152562429, 'mae_dropoffs_prophet': 14.345435894657854}\n",
      "TEST: {'rows': 2928, 'mae_pickups_sarimax': 16.392839568616655, 'mae_dropoffs_prophet': 16.798808070995065}\n"
     ]
    }
   ],
   "source": [
    "def summarize(split):\n",
    "    d = out[out[\"split\"] == split]\n",
    "    return {\n",
    "        \"rows\": len(d),\n",
    "        \"mae_pickups_sarimax\": float(d[\"ae_pickups_sarimax\"].mean()),\n",
    "        \"mae_dropoffs_prophet\": float(d[\"ae_dropoffs_prophet\"].mean()),\n",
    "    }\n",
    "\n",
    "print(\"VAL:\", summarize(\"val\"))\n",
    "print(\"TEST:\", summarize(\"test\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab7bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
